tmp = vector()
for (k in 1:5){
nestdata = plotdata%>%filter(NEST ==k)
tmp = c(tmp, nrow(nestdata))
}
plot_nest_df = rbind(plot_nest_df, tmp)
colnames(plot_nest_df) = c("Nest1","Nest2","Nest3","Nest4","Nest5")
}
site_list[[i]]= plot_nest_df
}
return(site_list)
}
richness = function(data){
site_list = list()
browser()
for (i in 1:103){
sitedata = data%>%filter(SITE == i)
plot_nest_df = data.frame()
for (j in 1:16){
plotdata = sitedata%>%filter(PLOT == j)
tmp = vector()
for (k in 1:5){
nestdata = plotdata%>%filter(NEST ==k)
tmp = c(tmp, nrow(nestdata))
}
plot_nest_df = rbind(plot_nest_df, tmp)
colnames(plot_nest_df) = c("Nest1","Nest2","Nest3","Nest4","Nest5")
}
site_list[[i]]= plot_nest_df
}
return(site_list)
}
sr = richness(Data_Yr2_veg)
richness = function(data){
site_list = list()
#browser()
for (i in 1:103){
sitedata = data%>%filter(SITE == i)
plot_nest_df = data.frame()
for (j in 1:16){
plotdata = sitedata%>%filter(PLOT == j)
tmp = vector()
for (k in 1:5){
nestdata = plotdata%>%filter(NEST ==k)
tmp = c(tmp, nrow(nestdata))
}
plot_nest_df = rbind(plot_nest_df, tmp)
colnames(plot_nest_df) = c("Nest1","Nest2","Nest3","Nest4","Nest5")
}
site_list[[i]]= plot_nest_df
}
return(site_list)
}
sr = richness(Data_Yr2_veg)
sr[[1]]
s = Data_Yr2_veg%>%filter(SITE == 8)
p = s%>%filter(PLOT == 2)
p
sr[[8]]
n = p%>%filter(NEST == 1)
n
sum(is.na(Data_Yr2_veg$NEST))
dim(Data_Yr2_veg)
Data_Yr2_veg%>%filter(Site == 96)%>% (PLOT == 4)
knitr::opts_chunk$set(echo = TRUE)
#clear the workspace
rm(list = ls())
cat("\014")
inputfile = 'LookUpSiteDescriptorCodes.csv'
fullfile = paste("../Data",inputfile, sep = "/")
LookUpSite = as.tbl(read.csv(fullfile))
knitr::opts_chunk$set(echo = TRUE)
#clear the workspace
rm(list = ls())
cat("\014")
inputfile = 'LookUpSiteDescriptorCodes.csv'
fullfile = paste("../Data",inputfile, sep = "/")
LookUpSite = as.tbl(read.csv(fullfile))
setwd("~/Documents/CMEECourseWork/CMEEMainProject/Code")
inputfile = 'LookUpSiteDescriptorCodes.csv'
fullfile = paste("../Data",inputfile, sep = "/")
LookUpSite = as.tbl(read.csv(fullfile))
library(dplyr)
library(ggplot2)
library(gridExtra)
inputfile = 'LookUpSiteDescriptorCodes.csv'
fullfile = paste("../Data",inputfile, sep = "/")
LookUpSite = as.tbl(read.csv(fullfile))
inputfile = 'Site_descriptors00-03.csv'
fullfile = paste("../Data",inputfile, sep = "/")
SiteDescriptorsYr2 = as.tbl(read.csv(fullfile))
PositiveCodesSites = c(8,9,10,12,13,14,15,16,17,18,24,54,55,56,57,58,59,61,62,63,64,65,86,87,88,89,90,91,92,93,105,106,107,108,109,110,111,112,113,114,115,116,150,151,152,153,157,158,208,209,210,211,212)
PositveSites = SiteDescriptorsYr2 %>% select(SITE, SD_code)%>% filter(SD_code %in% PositiveCodesSites)
library(dplyr)
library(ggplot2)
library(gridExtra)
inputfile = 'LookUpSiteDescriptorCodes.csv'
fullfile = paste("../Data",inputfile, sep = "/")
LookUpSite = as.tbl(read.csv(fullfile))
inputfile = 'Site_descriptors00-03.csv'
fullfile = paste("../Data",inputfile, sep = "/")
SiteDescriptorsYr2 = as.tbl(read.csv(fullfile))
PositiveCodesSites = c(8,9,10,12,13,14,15,16,17,18,24,54,55,56,57,58,59,61,62,63,64,65,86,87,88,89,90,91,92,93,105,106,107,108,109,110,111,112,113,114,115,116,150,151,152,153,157,158,208,209,210,211,212)
PositveSites = SiteDescriptorsYr2 %>% select(SITE, SD_code)%>% filter(SD_code %in% PositiveCodesSites)
read.csv("../Data/LookUpSiteDescriptorCodes.csv")
View(LookUpSite)
View(LookUpSite)
LookUpSiteCodes = read.csv("../Data/LookUpSiteDescriptorCodes.csv")
PositveSites = SiteDescriptorsYr2 %>% select(SITE, SD_code)%>% filter(SD_code %in% PositiveCodesSites)
positive_key = inner_join(LookUpSiteCodes,PositiveSites)
positive_key = inner_join(LookUpSiteCodes,PositveSites)
View(positive_key)
View(positive_key)
View(LookUpSiteCodes)
View(LookUpSiteCodes)
positive_key = inner_join(LookUpSiteCodes,PositveCodesSites)
positive_key = inner_join(LookUpSiteCodes,PositiveCodesSites)
positive_key = inner_join(LookUpSiteCodes,as.tble(PositiveCodesSites))
positive_key = inner_join(LookUpSiteCodes,as.tbl(PositiveCodesSites))
View(LookUpSite)
View(LookUpSite)
positive_key = subset(LookUpSiteCodes, SD_code %in% PositiveCodesSites)
View(positive_key)
View(positive_key)
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(ggplot2)
library(gridExtra)
# these are the positive codes used
LookUpSiteCodes = read.csv("../Data/LookUpSiteDescriptorCodes.csv")
positive_key = subset(LookUpSiteCodes, SD_code %in% PositiveCodesSites)
positive_key
# these are the negative codes used
negative_key = subset(LookUpSiteCodes, SD_code %in% NegativeCodesSites)
NegativeCodesSites = c(6,41,143,145,149,117,213,215)
NegativeSites = SiteDescriptorsYr2 %>% select(SITE, SD_code)%>% filter(SD_code %in% NegativeCodesSites)
# these are the negative codes used
negative_key = subset(LookUpSiteCodes, SD_code %in% NegativeCodesSites)
negative_key
# these are the negative codes used
negative_key = subset(LookUpSite, SD_code %in% NegativeCodesSites)
negative_key
# these are the positive codes used
positive_key = subset(LookUpSite, SD_code %in% PositiveCodesSites)
positive_key
# these are the positive codes used
positive_key = subset(LookUpSite, SD_code %in% PositiveCodesSites)
positive_key
write.csv(positive_key, "../Results/Positice_codes_used.csv")
# these are the positive codes used
positive_key = subset(LookUpSite, SD_code %in% PositiveCodesSites)
positive_key
write.csv(positive_key, "../Results/Positive_codes_used.csv")
# these are the negative codes used
negative_key = subset(LookUpSite, SD_code %in% NegativeCodesSites)
negative_key
write.csv(negative_key,"../Results/Negative_codes_used.csv")
knitr::opts_chunk$set(echo = FALSE,message = FALSE,fig.pos = "H" ,comment=NA, fig.align ="centre")
date = format(Sys.Date(), "%B %d %Y")
cat(date)
#This creates pdf from command line, note, sensitive to ' or "
#Rscript -e "library(knitr); knit('MiniProj2.Rmd')"
#Rscript -e "library(rmarkdown); render('MiniProj2.md')"
#clear the workspace
rm(list = ls())
cat("\014")
#setwd("~/Documents/CMEECourseWork/MiniProject/Code")
library(ggplot2)
library(reshape) # both required for the box plots, otherwise they cant all be presented
# on one page and therefore difficult to analyse
library(rpart)
library(rpart.plot)# both required for the decision tree
library(knitr)
library(kableExtra) # for kable stylig options, to hold position on page
#Get the data , enter input CSV file name here, for data in data directory
inputfile = 'SorariaCompact1.csv'
Dataname = strsplit(inputfile, "\\.")[[1]][[1]]
fullfile = paste("../Data",inputfile,sep = '/')
Data = read.csv(fullfile)
speciesnames = as.character(unique(unlist(Data$Species))) # uselful for nameing things
numspecies = summary(Data$Species) # useful for comparisons
#Median imputation
median_replace1 = function(x){
ifelse(is.na(x), median(x,na.rm = TRUE), x)
}
median_replace2 = function(x){
apply(x,2,median_replace1)
}
Imputed_list = lapply(split.data.frame(Data[,2:12], Data$Species), FUN = median_replace2)
#The imputed dataframe is a list with species as the elements, the following sticks it back together with a different name so both optiona are available
temp = do.call(rbind, Imputed_list)
Imputed_df = cbind(Data[1], temp)
# Some algorthms are sensitive to the scale of the data, so here the entire dataframe is scaled
Scaled_df = scale(Imputed_df[-1])
Scaled_df = cbind(Data[1], Scaled_df)
# but this might reduce the dissimilarity to much, so this is a semi-scaled datafrane.
temp = Imputed_df[-c(1,6,7,8,12)]
temp = scale(temp)
Semi_Scaled_df = cbind(Imputed_df[c(1,6:8,12)], temp)
# Model evaluation metrics
accuracy = function(atable){
a = round(sum(diag(atable)/sum(atable)), digits = 2)
return(a)
}
# precision = TP/( rest of that column in conf matrix = the other species id in same class)
precision = function(atable){
p = vector()
items = vector()
no_predictions = dim(atable)[2]
for (i in 1:no_predictions){
items[i] = paste("class",colnames(atable)[i], sep = "_")
p[i] = round(diag(atable)[i]/(sum((atable)[,i])), digits = 2)
}
precisions = cbind(items,p)
colnames(precisions) = c("Class", "Precision")
return(precisions)
}
#sensitivity = TP/ rest of that row = the other classes the algorithm has put species in
sensitivity = function(atable){
s = vector()
no_actuals = dim(atable)[1]
for (i in 1:no_actuals){
s[i] = round(diag(atable)[i]/(sum((atable)[i,])), digits = 2)
}
sensitivities = cbind(rownames(atable),s)
colnames(sensitivities) = c("Species", "Sensitivity")
return(sensitivities)
}
#Data sampling and test/train sets.
#This shuffles and splits the data
shuffle = function(dataset){
splits = list()
set.seed(42)
n = nrow(dataset)
shuffled = dataset[sample(n),]
train = shuffled[1:round(0.7*n),]
test = shuffled[(round(0.7*n)+1):n,]
splits[[1]] = train
splits[[2]] = test
return(splits)
}
#this subsets the data into species
create_train_test = function(dataset){
sets = as.character(unique(dataset[,1]))
train = data.frame()
test = data.frame()
split_data = list()
for (i in 1:length(sets)){
sub = subset(dataset, dataset[,1] == sets[i])
train_temp = shuffle(sub)[[1]]
test_temp = shuffle(sub)[[2]]
train = rbind(train, train_temp)
test = rbind(test, test_temp)
}
split_data[[1]] = train
split_data[[2]] = test
return(split_data)
}
#PS you can check the splits are correct with summary(train$species), summary(test$species)
#summary(maindata$species), this gives numbers in each species.
#to include a cross fold validation repeat above fold times
# performs the k means algorith over 10 repeats, returns BSS/Wss ratio, accuracy and
repeated_kmeans = function(dataset){
metrics_list = list()
accuracy_vector = vector()
ratio = vector()
species_no = data.frame(matrix(ncol = 7))
colnames(species_no) = speciesnames
sens = data.frame(row.names = speciesnames )
prec = data.frame(rownames = speciesnames)
for (i in 1:10){
kmeans_result = kmeans(dataset[-1], 7, 20, iter.max = 50, algorithm = "MacQueen")
ratio[i] = round(kmeans_result$tot.withinss/kmeans_result$totss, digits = 2)
kmeans_conf = table(Imputed_df$Species, kmeans_result$cluster)
accuracy_vector[i] = accuracy(kmeans_conf)
species = diag(kmeans_conf)
species_no = rbind(species_no, species)# just TP
s = sensitivity(kmeans_conf)
sens = cbind(sens, s[,2])
p = precision(kmeans_conf)
prec = cbind(prec,p[,2])
}
metrics_list[[1]] = ratio # wss/bss
metrics_list[[2]] = accuracy_vector #sum TP/no things done
metrics_list[[3]] = species_no[-1,]
metrics_list[[4]] = sens
metrics_list[[5]] = prec
return(metrics_list)
}
# data exploration - box plots
melted = melt(Scaled_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
theme(axis.ticks = element_blank(), axis.text.x = element_blank())
# data exploration - box plots
melted = melt(Imputed_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
theme(axis.ticks = element_blank(), axis.text.x = element_blank())
#Getting the results for the kmeans
#Imputed df without scaling
Imputed_kmeans = repeated_kmeans(Imputed_df)
#Semi scaled data
#Semi_scaled_kmeans = repeated_kmeans(Semi_Scaled_df)
# fully scaled data
Scaled_kmeans = repeated_kmeans(Scaled_df)
#Disaply BSS/WSS ratio for the kmeans calculated in chunk above
SS_df = data.frame(nrow = 2)
SS_df = rbind(Imputed_kmeans[[1]],Scaled_kmeans[[1]])
rownames(SS_df) = c("unstandardized","standardized")
colnames(SS_df) = c("Run 1","Run 2","Run 3","Run 4","Run 5","Run 6","Run 7", "Run 8","Run 9","Run 10")
kable(SS_df, format = "latex", caption = "Within cluster to between cluster ratio")%>%
kable_styling(latex_options = "hold_position")
#Display accuaracy for kmeans calcualted above. this is accuarcy, not repcision
acc_df = data.frame(nrow = 2)
acc_df = rbind(Imputed_kmeans[[2]],Scaled_kmeans[[2]])
rownames(acc_df) = c("unstandardized","standardized")
colnames(acc_df) = c("Run 1","Run 2","Run 3","Run 4","Run 5","Run 6","Run 7", "Run 8","Run 9","Run 10")
kable(acc_df, format = "latex", caption = "Accuracy")%>%
kable_styling(latex_options = "hold_position")
#Display percentage of true positives from the confusion matrix calcualted in kmeans chunk above
m1 = Imputed_kmeans[[3]]
#m2 = Semi_scaled_kmeans[[3]]
m3 = Scaled_kmeans[[3]]
m1_percent = round(apply(m1, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m1_percent) = c(1:10)
kable(m1_percent, format = "latex", caption = "Percentage of true positives for non-standarized data")%>%
kable_styling(latex_options = "hold_position")
m3_percent = round(apply(m3, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m3_percent) = c(1:10)
kable(m3_percent, format = "latex", caption = "Percentage of true positives for standarized data")%>%
kable_styling(latex_options = "hold_position")
prec_df_Imputed = Imputed_kmeans[[4]]
colnames(prec_df_Imputed) = c(1:10)
kable(prec_df_Imputed, format = "latex", caption = "Precision of kmeans with non standardized data")%>%
kable_styling(latex_options = "hold_position")
prec_df = Scaled_kmeans[[4]]
colnames(prec_df) = c(1:10)
kable(prec_df, format = "latex", caption = "Precision of kmeans with standardized data")%>%
kable_styling(latex_options = "hold_position")
sens_df_Imputed = Imputed_kmeans[[4]]
colnames(sens_df_Imputed) = c(1:10)
kable(sens_df_Imputed, format = "latex", caption = "Sensitivity of kmeans with non standardized data")%>%
kable_styling(latex_options = "hold_position")
sens_df_scaled = Scaled_kmeans[[4]]
colnames(sens_df_scaled) = c(1:10)
kable(sens_df_scaled, format = "latex", caption = "Sensitivity of kmeans with standardized data")%>%
kable_styling(latex_options = "hold_position")
# reapeats over the 5 methodsa ad returns confusion matrix for each, plus accuracy for each
repeated_hclust = function(dataset){
conf = list()
metrics_list = list()
accuracy_vector = vector()
dist_methods = c("euclidean", "maximum","manhattan","canberra","minkowski")
for (i in 1:length(dist_methods)){
method = dist_methods[i]
distance = dist(dataset[-1], method = method)
hcluster = hclust(distance, method = "complete")
cluster = cutree(hcluster, k = 7)
conf[[i]] = table(dataset$Species, cluster)
accuracy_vector[i] = accuracy(conf[[i]])
}
accuracy_df = rbind(dist_methods, accuracy_vector)
metrics_list[[1]]=accuracy_df
metrics_list[[2]]=conf
names(metrics_list)=c("Accuracy", "Confusion Matrix")
return(metrics_list)
}
hcluster = repeated_hclust(Imputed_df)
accs = data.frame(nrow = 2)
accs = rbind(hcluster[[1]][1,], hcluster[[1]][2,])
rownames(accs) = c("Distance Method", "Accuracy")
colnames(accs) = c(" ", " "," ", " "," ")
kable(accs, format = "latex", caption = "Accuracy obtained in hierarchical clustering using different distance metrics for non standarized data")%>%
kable_styling(latex_options = "hold_position")
hcluster_scaled = repeated_hclust(Scaled_df)
accs = data.frame(nrow = 2)
accs = rbind(hcluster_scaled[[1]][1,], hcluster_scaled[[1]][2,])
rownames(accs) = c("Distance Method", "Accuracy")
colnames(accs) = c(" ", " "," ", " "," ")
kable(accs, format = "latex", caption = "Accuracy obtained in hierarchical clustering using different distance metrics for standardized data")%>%
kable_styling(latex_options = "hold_position")
kable(hcluster[[2]][[4]], format = "latex", caption = "Confusion matrix for Canberra method using un standardized data")%>%
kable_styling(latex_options = "hold_position")
kable(hcluster_scaled[[2]][[4]], format = "latex", caption = "Confusion matrix for Canberra method with standardized data")%>%
kable_styling(latex_options = "hold_position")
prec_imp = precision(hcluster[[2]][[2]])
Standardized_prec = prec_imp[,2]
prec_sc = precision(hcluster[[2]][[4]])
Non_standardized_prec = prec_sc[,2]
sens_imp = sensitivity(hcluster[[2]][[4]])
Standardized_sens = sens_imp[,2]
sens_sc = sensitivity((hcluster_scaled[[2]][[4]]))
Non_standardized_sens = sens_sc[,2]
precision_table = cbind(Standardized_prec, Non_standardized_prec)
rownames(precision_table) = c("Class1", "Class2","Class3","Class4","Class5","Class6","Class7")
colnames(precision_table) = c("Standardized", "Unstandardized")
sensitivity_table = cbind(Standardized_sens, Non_standardized_sens)
rownames(sensitivity_table) = c(speciesnames)
colnames(sensitivity_table) = c("Standardized", "Non-standardized")
kable(precision_table, format = "latex", caption = "Precision for standardized and non-standardized data")%>%
kable_styling(latex_options = "hold_position")
kable(sensitivity_table, format = "latex", caption = "Precision for standardized and non-standardized data")%>%
kable_styling(latex_options = "hold_position")
Imputed_sets = create_train_test(Imputed_df)
Imputed_train = Imputed_sets[[1]]
Imputed_test = Imputed_sets[[2]]
tree = rpart(Species~., Imputed_train, method = "class", control = rpart.control(cp = 0.00001))
rpart.plot(tree, box.palette = "Blues",fallen.leaves = FALSE,  gap=0, space=0)
pred_tree = predict(tree, Imputed_test, type = "class")
confusion_tree = table(Imputed_test$Species, pred_tree)
cat("The accuracy for the decision tree is ")
cat(accuracy(table(Imputed_test$Species, pred_tree)))
sens_tree = (sensitivity(table(Imputed_test$Species, pred_tree)))
kable(sens_tree, format = "latex", caption = "Sensitivity for the decision tree")%>%
kable_styling(latex_options = "hold_position")
prec_tree = precision(table(Imputed_test$Species, pred_tree))
kable(prec_tree, format = "latex", caption = "Precision for the decision tree")%>%
kable_styling(latex_options = "hold_position")
kable(cbind(table(Imputed_test$Species, pred_tree),summary(Imputed_test$Species)), format = "latex", caption =  "Confusion matrix for the decision tree")%>%
kable_styling(latex_options = "hold_position")
#Summarise all the precisions and sensitivity
sens_totals = cbind(sensitivity_table, sens_tree[,2])
colnames(sens_totals) = c("hclust standardised", "hclust non-standardized", "tree")
kable(sens_totals, format = "latex", caption = "Sensitivity for hierarchical clustering and decision tree")%>%
kable_styling(latex_options = "hold_position")
prec_totals = cbind(precision_table, prec_tree[,2])
colnames(prec_totals) = c("hclust standardised", "hclust non-standardized", "tree")
kable(prec_totals, format = "latex", caption = "Precision for hierarchical clustering and decision tree")%>%
kable_styling(latex_options = "hold_position")
#Disaply BSS/WSS ratio for the kmeans calculated in chunk above
SS_df = data.frame(nrow = 2)
SS_df = rbind(Imputed_kmeans[[1]],Scaled_kmeans[[1]])
rownames(SS_df) = c("unstandardized","standardized")
colnames(SS_df) = c("Run 1","Run 2","Run 3","Run 4","Run 5","Run 6","Run 7", "Run 8","Run 9","Run 10")
kable(SS_df, format = "latex", caption = "Within cluster to between cluster ratio")%>%
kable_styling(latex_options = "hold_position")
numspecies
summary(train$species)
create_train_test = function(dataset){
sets = as.character(unique(dataset[,1]))
train = data.frame()
test = data.frame()
split_data = list()
for (i in 1:length(sets)){
sub = subset(dataset, dataset[,1] == sets[i])
train_temp = shuffle(sub)[[1]]
test_temp = shuffle(sub)[[2]]
train = rbind(train, train_temp)
test = rbind(test, test_temp)
summary(test$species)
}
split_data[[1]] = train
split_data[[2]] = test
return(split_data)
}
#Data sampling and test/train sets.
#This shuffles and splits the data
shuffle = function(dataset){
splits = list()
set.seed(42)
n = nrow(dataset)
shuffled = dataset[sample(n),]
train = shuffled[1:round(0.7*n),]
test = shuffled[(round(0.7*n)+1):n,]
splits[[1]] = train
splits[[2]] = test
return(splits)
}
#this subsets the data into species
create_train_test = function(dataset){
sets = as.character(unique(dataset[,1]))
train = data.frame()
test = data.frame()
split_data = list()
for (i in 1:length(sets)){
sub = subset(dataset, dataset[,1] == sets[i])
train_temp = shuffle(sub)[[1]]
test_temp = shuffle(sub)[[2]]
train = rbind(train, train_temp)
test = rbind(test, test_temp)
}
split_data[[1]] = train
split_data[[2]] = test
return(split_data)
}
#PS you can check the splits are correct with summary(train$species), summary(test$species)
#summary(maindata$species), this gives numbers in each species.
#to include a cross fold validation repeat above fold times
numspecies
sum(numspecies)
percent = numspecies/380
percent
speciespropn = numspecies/380
speciespropn = numspecies/380
speciespropn
speciespropn = round(numspecies/380, digits = 2)
speciespropn
speciespropn = round(numspecies/380, digits = 2)
speciespropn
speciespropn = round(numspecies/380, digits = 2)
kable(speciespropn, format = "latex", caption = "Proportions of each species in test set")%>%
kable_styling(latex_options = "hold_position")
#cat("The accuracy for the decision tree is ")
#cat(accuracy(table(Imputed_test$Species, pred_tree)))
tree_acc = accuracy(table(Imputed_test$Species, pred_tree))
tree_acc
pred_tree = predict(tree, Imputed_test, type = "class")
confusion_tree = table(Imputed_test$Species, pred_tree)
#cat("The accuracy for the decision tree is ")
#cat(accuracy(table(Imputed_test$Species, pred_tree)))
tree_acc = accuracy(table(Imputed_test$Species, pred_tree))
kable(tree_acc, format = "latex", caption = "Accuracy of the decision tree")%>%
kable_styling(latex_options = "hold_position")
