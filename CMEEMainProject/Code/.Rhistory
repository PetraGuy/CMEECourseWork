#lapply works over this list
#apply works over the cols within each dataframe of the list
MedianImputedDF <- lapply(split.data.frame(soraria_df[,2:9], soraria_df$Species), FUN = medianreplace2)
#Above is a list of dataframes, each element is a species,
#to join them back up row by row, with col 1 is species do this
temp <- do.call(rbind, MedianImputedDF) #PS unplsit did not work, needed this do. call thing
SorariaImpute <- cbind(soraria_df[1], temp)
# Now I need to unsplit the "newthing"
# this calculates the medians and gives you a dataframe, just so you know them
#doesnt replace
#
median1 <- function(x) {
median(x, na.rm = TRUE)
}
mediansAre <- aggregate(soraria_df[,2:9], soraria_df[1], median1)
set.seed(1)
# Chop up soraria in my_soraria and species
my_soraria <- as.matrix(SorariaImpute[-1])
species <- soraria_df$Species
# Perform k-means clustering on my_soraria: kmeans_soraria
kmeans_soraria <- kmeans(my_soraria, 6)
# Compare the actual Species to the clustering using table()
table(species, kmeans_soraria$cluster)
plot(Petal.Length ~ Petal.Width, data = my_soraria, col = kmeans_soraria$cluster)
View(my_soraria)
View(my_soraria)
plot(LeafLength ~ LeafWidth, data = my_soraria, col = kmeans_soraria$cluster)
lapply(MedianImputedDF[-1], pairs)
lapply(MedianImputedDF[-1], pairs)
#a random forest train needs caret
library(caret)
Y = soraria_df[,1]
X = soraria_df[,2:11]
random_forest_soraria <-  train(X, Y,
#tuneGrid = data.frame(mtry = c(2, 3, 7)),
data = soraria_df,
method = "ranger",
preProcess = c("medianImpute","center","scale"),
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE))
??train
library(caret)
library("caret")
library(caret)
install.packages(caret)
install.packages("caret")
library("caret")
library(caret)
Y = soraria_df[,1]
X = soraria_df[,2:11]
random_forest_soraria <-  train(X, Y,
#tuneGrid = data.frame(mtry = c(2, 3, 7)),
data = soraria_df,
method = "ranger",
preProcess = c("medianImpute","center","scale"),
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE))
ggplot(soraria_df, aes(x = LeafLength, y = LeafWidth, col = Species)) +
geom_point()
set.seed(1)
# Chop up soraria in my_soraria and species
my_soraria <- as.matrix(SorariaImpute[-1])
species <- soraria_df$Species
# Perform k-means clustering on my_soraria: kmeans_soraria
kmeans_soraria <- kmeans(my_soraria, 6)
# Compare the actual Species to the clustering using table()
table(species, kmeans_soraria$cluster)
plot(LeafLength ~ LeafWidth, data = my_soraria, col = kmeans_soraria$cluster)
library(ggplot2)
set.seed(1)
# Chop up soraria in my_soraria and species
my_soraria <- as.matrix(SorariaImpute[-1])
species <- soraria_df$Species
# Perform k-means clustering on my_soraria: kmeans_soraria
kmeans_soraria <- kmeans(my_soraria, 6)
# Compare the actual Species to the clustering using table()
table(species, kmeans_soraria$cluster)
plot(LeafLength ~ LeafWidth, data = my_soraria, col = kmeans_soraria$cluster)
plot(LeafLength ~ LeafWidth, data = my_soraria, col = kmeans_soraria$cluster)
#Import Soraria data set
dev.off()       # close current plots
rm(list = ls()) # clear env
cat("\014")     # clear console
library("openxlsx")
# workbook details
path = "C:/Users/Petra Guy/Google Drive/R/Projects/Sorbus/DataFiles"
file = "SorariaComplete.xlsx"
sheets = c("Anglica",
"Cuneifolia",
"Intermedia",
"Leyana",
"Minima",
"Mougeotii")
# join path and filename
pathfile = paste(path, file, sep="/")
# open the file with this package
xlsxFile = loadWorkbook(file = pathfile)
soraria_df = data.frame()
for (sheet in sheets){
# load data into dataframe
df = read.xlsx(xlsxFile = xlsxFile, sheet = sheet, skipEmptyRows = FALSE)
soraria_df = rbind(soraria_df, df)
}
# calculate medians and fill in NAs for each group with median
#this bit says if is NA, calculate a median
medianreplace1 <- function(x) {
ifelse(is.na(x), median(x, na.rm = TRUE), x)
}
#this bit does above, but in an apply
medianreplace2 <- function(x){
apply(x, 2,medianreplace1)
}
# now use that function in aggregate to calc median
#split the dataframe by species into a list of dataframes per species
#lapply works over this list
#apply works over the cols within each dataframe of the list
MedianImputedDF <- lapply(split.data.frame(soraria_df[,2:9], soraria_df$Species), FUN = medianreplace2)
#Above is a list of dataframes, each element is a species,
#to join them back up row by row, with col 1 is species do this
temp <- do.call(rbind, MedianImputedDF) #PS unplsit did not work, needed this do. call thing
SorariaImpute <- cbind(soraria_df[1], temp)
set.seed(1)
# Chop up soraria in my_soraria and species
my_soraria <- as.matrix(SorariaImpute[-1])
species <- soraria_df$Species
# Perform k-means clustering on my_soraria: kmeans_soraria
kmeans_soraria <- kmeans(my_soraria, 6)
# Compare the actual Species to the clustering using table()
table(species, kmeans_soraria$cluster)
plot(LeafLength ~ LeafWidth, data = my_soraria, col = kmeans_soraria$cluster)
plot(FruitLength ~ FruitWidth, data = my_soraria, col = kmeans_soraria$cluster)
View(my_soraria)
View(my_soraria)
MedianImputedDF
iris
clusters <- hclust(dist(my_soraria[,2:3]))
plot(clusters)
clusters <- hclust(dist(my_soraria[,2:3]), method = 'average')
plot(clusters)
# Apply median imputation: model
soraria_x <- as.matrix(soraria_df[,2:11])
soraria_y <- as.matrix(soraria_df[1])
myControl <- trainControl(
method = "cv", number = 10,
#summaryFunction = twoClassSummary,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE
)
model_knn <- train(
x = soraria_x, y = soraria_y,
method = "knn",
trControl = myControl,
preProcess = "medianImpute"
)
# Print model to console
model_knn
model_rf <- train(
x = soraria_x, y = soraria_y,
method = "ranger",
trControl = myControl,
preProcess = "medianImpute"
)
# Print model to console
model_knn
model_rf
parts <- colnames(soraria_df)
expvars <- as.vector(parts[-1])
library(rpart)
# using a tree model from intro to machine learning, lesson 12
tree <- rpart(Species ~ LeafLength + LeafWidth + WidestPoint + Angle,
data = soraria_df, method = "class")
# A dataframe containing unseen observations
#unseen <- data.frame(Sepal.Length = c(5.3, 7.2),
##  Sepal.Width = c(2.9, 3.9),
Petal.Length = c(1.7, 5.4),
# Petal.Width = c(0.8, 2.3))
# Predict the label of the unseen observations. Print out the result.
predict(tree, unseen, type = 'class')
model_knn
model_rf
parts <- colnames(soraria_df)
expvars <- as.vector(parts[-1])
library(rpart)
# using a tree model from intro to machine learning, lesson 12
tree <- rpart(Species ~ LeafLength + LeafWidth + WidestPoint + Angle,
data = soraria_df, method = "class")
# A dataframe containing unseen observations
#unseen <- data.frame(Sepal.Length = c(5.3, 7.2),
##  Sepal.Width = c(2.9, 3.9),
#  Petal.Length = c(1.7, 5.4),
# Petal.Width = c(0.8, 2.3))
# Predict the label of the unseen observations. Print out the result.
predict(tree, unseen, type = 'class')
# Print model to console
model_knn
model_rf
parts <- colnames(soraria_df)
expvars <- as.vector(parts[-1])
library(rpart)
# using a tree model from intro to machine learning, lesson 12
tree <- rpart(Species ~ LeafLength + LeafWidth + WidestPoint + Angle,
data = soraria_df, method = "class")
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
p = seq(from 0.1, to 0.9, by 0.1)
p = seq(from 0.1, to = 0.9, by 0.1)
p = seq(from 0.1, to = 0.9, by = 0.1)
p = seq(from= 0.1, to = 0.9, by = 0.1)
prior = c(rep(0.6, 4), 0.52, rep(0.6,4))
likelihood = dbinom(4, size = 20, prob = p)
p = seq(from= 0.1, to = 0.9, by = 0.1)
prior = c(rep(0.6, 4), 0.52, rep(0.6,4))
likelihood = dbinom(4, size = 20, prob = p)
print(likelihood)
?round
p = seq(from= 0.1, to = 0.9, by = 0.1)
prior = c(rep(0.6, 4), 0.52, rep(0.6,4))
likelihood = dbinom(4, size = 20, prob = p)
print(round(likelihood, digits = 3)
p = seq(from= 0.1, to = 0.9, by = 0.1)
prior = c(rep(0.6, 4), 0.52, rep(0.6,4))
likelihood = dbinom(4, size = 20, prob = p)
print(round(likelihood, digits = 3))
p = seq(from= 0.1, to = 0.9, by = 0.1)
prior = c(rep(0.6, 4), 0.52, rep(0.6,4))
likelihood = dbinom(4, size = 20, prob = p)
print(round(likelihood, digits = 3))
numerator = prior*likelihood
denominator = sum(numerator)
posterior = numerator/denominator
sum(posterior)
p = seq(from= 0.1, to = 0.9, by = 0.1)
prior = c(rep(0.6, 4), 0.52, rep(0.6,4))
likelihood = dbinom(4, size = 20, prob = p)
print(round(likelihood, digits = 3))
numerator = prior*likelihood
denominator = sum(numerator)
posterior = numerator/denominator
sum(posterior)
print(posterior)
citation(package = "MuMIn")
citation
citation(package = "randomForest")
citation(package = "gbm")
citation(package = "zetadiv")
install.packages("zetadiv")
citation(package = "zetadiv")
## NB - aseveral libraries mask each other here - arm masks dplyr and corrplot, therefore open libraries #as required.
knitr::opts_chunk$set(
echo = FALSE,
message = FALSE,
warning = FALSE
)
# Richness modelling
rm(list = ls())
cat("\014")
library(dplyr) # everything
library(ggplot2)
library(car) # for vif
library(reshape) # melt
# get the data
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
nestZs = readRDS("../nest_mixed_model_fits.RDS")
Zs = nestZs%>%select(Site,slope)
zeta_r = readRDS("../Zeta/zeta_r")
Site = c(1:103)
zeta_r = as.data.frame(cbind(Site,zeta_r))
site_data = inner_join(site_data,zeta_r)
#mean impute the missing PHI
meanPHI = round(mean(site_data$Pos_Hetero_Index, na.rm = TRUE),2)
x = site_data$Pos_Hetero_Index
x[is.na(x)] = meanPHI
site_data$Pos_Hetero_Index = x
site_data_zs = inner_join(site_data,Zs)
# slect only required variables
subset_all = site_data_zs%>%select("Site","slope","Area_ha",
"Northing", "Pos_Hetero_Index","Buffer3",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_treedensity","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meantreedensity")
## NB - several libraries mask each other here - arm masks dplyr and corrplot, therefore open libraries #as required.
knitr::opts_chunk$set(
echo = FALSE,
message = FALSE,
warning = FALSE
)
# Richness modelling
rm(list = ls())
cat("\014")
library(dplyr) # everything
library(ggplot2)
library(car) # for vif
library(reshape) # melt
# get the data
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
PlotZdata = read.csv("../../Data/z_ave_fits.csv")
Zs = PlotZdata%>%select(Site, slope)
zeta_r = readRDS("../Zeta/zeta_r")
Site = c(1:103)
zeta_r = as.data.frame(cbind(Site,zeta_r))
site_data = inner_join(site_data,zeta_r)
#mean impute the missing PHI
meanPHI = round(mean(site_data$Pos_Hetero_Index, na.rm = TRUE),2)
x = site_data$Pos_Hetero_Index
x[is.na(x)] = meanPHI
site_data$Pos_Hetero_Index = x
site_data_zs = inner_join(site_data,Zs)
#add in the new random path z's AND expectation method Zs
rand_zs = read.csv("../../Data/rand_zs.csv")
colnames(rand_zs) = c("Site","zr")
exp_zs = read.csv("../../Data/zexp.csv")
colnames(exp_zs) = c("Site","expz")
site_data_zs_zr=inner_join(site_data_zs,rand_zs)
site_data_allzs = inner_join(site_data_zs_zr,exp_zs)
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_allzs%>%select(Site,expz,Area_ha,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
#zeta_r"
colnames(subset_all) = c("Site","z","Area",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meanTD","zeta_r")
#,"zeta_r"
#remove the wood with the largest area
largest_area = as.numeric(subset_all%>%filter(Area == max(Area))%>%select(Site))
site_data_outlier1 = subset_all%>%filter(Site!=largest_area)
site_data_outlier1 = site_data_outlier1[,-3] # remove area column now
#remove the outlier in PHI
largest_PHI = as.numeric(subset_all%>%filter(PHI == max(PHI))%>%select(Site))
site_data_outlier2 = site_data_outlier1%>%filter(Site!=largest_PHI)
largest_PHI = as.numeric(site_data_outlier2%>%filter(PHI == max(PHI))%>%select(Site))
site_data_outlier3 = site_data_outlier2%>%filter(Site!=largest_PHI)
subset_sd = site_data_outlier3%>%select(Site,z,
Northing, PHI,Buffer,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_TD,area_ratio,zeta_r)
#,"zeta_r"
subset_mean = site_data_outlier3%>%select(Site,z,
Northing, PHI,meandbh,
meanph, Buffer, meanSOM,meanLBA,
meanTD,area_ratio, no_NVC,
no_MSG,zeta_r)
#,"zeta_r"
#create the model
mod_mean = lm(z~., data=rescaled_mean_data, na.action = "na.fail")
# we know richness vs ph usually unimodal around .5, therefore fit to meanpH^2
data = subset_mean[,-1]
z = subset_mean[,2]
#data$meanph = (data$meanph)^2
#rescale the data
library(arm) #for standarize
rescaled_mean_data = apply(data[,-1],2, rescale)
rescaled_mean_data = as.data.frame(cbind(z, rescaled_mean_data))
#create the model
mod_mean = lm(z~., data=rescaled_mean_data, na.action = "na.fail")
# we know richness vs ph usually unimodal around .5, therefore fit to meanpH^2
data = subset_sd[,-1]
z = subset_mean[,2]
#rescale the data
library(arm) #for standarize
rescaled_sd_data = apply(data[,-1],2, rescale)
rescaled_sd_data = as.data.frame(cbind(z, rescaled_sd_data))
#create the model
mod_sd = lm(z~., data=rescaled_sd_data, na.action = "na.fail")
library(MuMIn) #dredge and avg
#get top models
models = dredge(mod_mean)
model_set = get.models(models, subset = delta<1.5)
#do model averaging, subset means zero method
mean_avg_models = model.avg(model_set, subset)
#select output data
summary = summary(mean_avg_models)
coefs =  mean_avg_models$coefficients
importance =  c(NA,(as.vector(mean_avg_models$importance)))
coef_matrix = summary$coefmat.full
coefs = coef_matrix[,1]
adj_se = coef_matrix[,3]
CI_lower =  coefs - 1.96*adj_se
CI_upper = coefs + 1.96*adj_se
output = round(as.data.frame(cbind(coefs,adj_se, CI_lower, CI_upper, importance)),2)
# make plot of the variables and CI
n = nrow(output)
data = output[c(2:n),]
data = data[,c(1,3,4)]
data = t(data)
data = as.data.frame(data)
melted = melt(data)
fun_mean <- function(x){
data = data.frame(y=mean(x),label=mean(x,na.rm=T))
data = round(data,2)
return(data)}
#(aes_string(x = 'variable', y='value', na.rm = TRUE)
melted$variable = as.factor(melted$variable)# ps, wont plot as separate plots if x continous
ggplot(melted,aes_string(x = 'variable', y ='value' ) )+
geom_boxplot(na.rm = TRUE, width = 0)+
stat_summary(fun.y = mean, geom="point",colour="darkred", size=3) +
stat_summary(fun.data = fun_mean, geom="text", vjust=-0.7)+
stat_summary(geom = "text", label = output$importance[-1],fun.y = max, hjust = 1, colour = "red")+
geom_abline(intercept = 0, slope = 0)+
labs(y = "Effect size and 95%CI",x = "effect")+
labs(title = "Model averaged results for delta <1.5, Plot Zs, Mean dataset",
subtitle = "numbers in red are variable importance")
library(MuMIn) #dredge and avg
#get top models
models = dredge(mod_sd)
model_set = get.models(models, subset = delta<1.5)
#do model averaging, subset means zero method
sd_avg_models = model.avg(model_set, subset)
#select output data
summary = summary(sd_avg_models)
coefs =  sd_avg_models$coefficients
importance =  c(NA,(as.vector(sd_avg_models$importance)))
coef_matrix = summary$coefmat.full
coefs = coef_matrix[,1]
adj_se = coef_matrix[,3]
CI_lower =  coefs - 1.96*adj_se
CI_upper = coefs + 1.96*adj_se
output = round(as.data.frame(cbind(coefs,adj_se, CI_lower, CI_upper, importance)),2)
# make plot of the variables and CI
n = nrow(output)
data = output[c(2:n),]
data = data[,c(1,3,4)]
data = t(data)
data = as.data.frame(data)
melted = melt(data)
fun_mean <- function(x){
data = data.frame(y=mean(x),label=mean(x,na.rm=T))
data = round(data,2)
return(data)}
#(aes_string(x = 'variable', y='value', na.rm = TRUE)
melted$variable = as.factor(melted$variable)# ps, wont plot as separate plots if x continous
ggplot(melted,aes_string(x = 'variable', y ='value' ) )+
geom_boxplot(na.rm = TRUE, width = 0)+
stat_summary(fun.y = mean, geom="point",colour="darkred", size=3) +
stat_summary(fun.data = fun_mean, geom="text", vjust=-0.7)+
stat_summary(geom = "text", label = output$importance[-1],fun.y = max, hjust = 1, colour = "red")+
geom_abline(intercept = 0, slope = 0)+
labs(y = "Effect size and 95%CI",x = "effect")+
labs(title = "Model averaged results for delta <1.5, Plot Zs, SD dataset",
subtitle = "numbers in red are variable importance")
#extract model averaged parameter estimates
predicted_Z = predict(mean_avg_models, full = TRUE)
empirical_Z =  site_data_outlier3$z
fit = lm(empirical_Z ~ predicted_Z)
R2 = round(summary(fit)$r.squared,2)
subtitle = paste("R2 = ",R2)
data = as.data.frame(cbind(predicted_Z, empirical_Z))
ggplot(data, aes(x = predicted_Z, y = empirical_Z))+
geom_point()+
geom_abline(intercept = 0, slope = 1)+
labs(title = "Observed versus predicted data, mean dataset",
subtitle = subtitle)
#extract model averaged parameter estimates
predicted_Z = predict(sd_avg_models, full = TRUE)
empirical_Z =  site_data_outlier3$z
fit = lm(empirical_Z ~ predicted_Z)
R2 = round(summary(fit)$r.squared,2)
subtitle = paste("R2 = ",R2)
data = as.data.frame(cbind(predicted_Z, empirical_Z))
ggplot(data, aes(x = predicted_Z, y = empirical_Z))+
geom_point()+
geom_abline(intercept = 0, slope = 1)+
labs(title = "Observed versus predicted data, sd dataset",
subtitle = subtitle)
library(dplyr)
sites = read.csv("../Data/SiteVars.csv")
scotswoods = c(38,40,42,47,50,53,56,59,68,74,76,77,98)
scots = sites%>%filter(Site%in%scotswoods)
southern = sites%>%filter(!Site%in%scotswoods)
awscot = sum(scots$AW)
pawscot = sum(scots$PAWS)
awsouthern= sum(southern$AW)
pawsouthern = sum(southern$PAWS)
anyscots = sum(scots$Any.Anc)
anysouthern = sum(southern$Any.Anc)
library(dplyr)
library(dplyr)
setwd("C:/dev/code/CMEECourseWork/CMEEMainProject/Code")
sites = read.csv("../Data/SiteVars.csv")
scotswoods = c(38,40,42,47,50,53,56,59,68,74,76,77,98)
scots = sites%>%filter(Site%in%scotswoods)
southern = sites%>%filter(!Site%in%scotswoods)
awscot = sum(scots$AW)
pawscot = sum(scots$PAWS)
awsouthern= sum(southern$AW)
pawsouthern = sum(southern$PAWS)
anyscots = sum(scots$Any.Anc)
anysouthern = sum(southern$Any.Anc)
View(scots)
View(sites)
Data = read.csv("../Data/GroundCover.csv")
Data_Yr2 = Data%>%filter(Yr_2 == 2)#%>%select(SITE,PLOT,NEST,COV,Amalgams)
colnames(Data_Yr2) = c("SITE", "PLOT","NEST","Cover","BRC_number","Year")
veg_codes = read.csv("../Data/vegetation_codes.csv")
colnames(veg_codes) = c("Species", "BRC_number")
Data_Yr2_veg = Data_Yr2%>% inner_join(veg_codes)
Sitedata =  read.csv("../Data/CompleteSiteLevelVars.csv")
View(Sitedata)
View(Sitedata)
min = Sitedata%>%filter(Richness = min(Richness))
min = Sitedata%>%filter(Richness == min(Richness))
min
max = Sitedata%>%filter(Richness == max(Richness))
max
View(sites)
