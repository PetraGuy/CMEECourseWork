---
title: "BoosetedTreeModels"
author: "Petra Guy"
date: "17 May 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
rm(list = ls())
cat("\014")
library(dplyr)
library(rpart)
library(rpart.plot)
library(gbm)#bgm
library(caret)
library(Metrics) #rmse
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
site_data = site_data[,-1]

zeta_r = readRDS("../Zeta/zeta_r")
Site = c(1:103)
zeta_r = as.data.frame(cbind(Site,zeta_r))
site_data = inner_join(zeta_r,site_data)

#mean impute the missing PHI
meanPHI = round(mean(site_data$Pos_Hetero_Index, na.rm = TRUE),2)
x = site_data$Pos_Hetero_Index
x[is.na(x)] = meanPHI
site_data$Pos_Hetero_Index = x

subset_all = site_data%>%select("Site","Richness","Area_ha",
                                "Northing", "Pos_Hetero_Index","Buffer3",
                                "no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
                                "sd_meandbh","sd_treedensity","area_ratio",
                                "meandbh","meanph", "meanSOM","meanLBA",
                                "meantreedensity","zeta_r")



colnames(subset_all) = c("Site","Richness","Area",
                         "Northing", "PHI","Buffer",
                         "no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
                         "sd_meandbh","sd_TD","area_ratio",
                         "meandbh","meanph", "meanSOM","meanLBA",
                         "meanTD","zeta_r")
```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
subset_sd = subset_all%>%select("Richness",
                               "PHI","Buffer",
                               "no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
                               "sd_meandbh","sd_TD","area_ratio","zeta_r")



subset_mean = subset_all%>%select("Richness","meanph",
                              "PHI",  "meandbh","meanSOM",
                               "Buffer", "meanLBA",
                              "meanTD","area_ratio", "no_NVC", 
                              "no_MSG","zeta_r")


```


```{r}
#Using bootstrapping and CV but without train an test set (see Kuhn p67)
#Do a by hand tune first to explore what's might happen at the limits

#gbm we can tune:n.trees,interaction.depth,shrinkage,n.minobsinnode


  set.seed(1)
 model = gbm(formula = Richness~.,
              data = subset_mean,
              n.minobsinnode = 5,
              bag.fraction = 0.8 ,
              interaction.depth =5,
               n.trees = 10000,
               distribution = "gaussian",
               cv.folds =5)
print( model$train.error[length(model$train.error)])  


```

```{r}
pred <- predict(model,subset_mean,10000)
rmse(actual = subset_mean$Richness,
     predicted = pred)
```

```{r}
summary(model,n.trees=10000)
```



```{r}
#redo above in a loop

nodes  = seq(2,10,1)
idepth = seq(2,6,1)
bag = seq(0.5,0.8,0.1)
hyper_grid = expand.grid(nodes = nodes, bag = bag, idepth = idepth)

oob_err = c()

for (i in 1:nrow(hyper_grid)){
 # browser()
  set.seed(1)
 model = gbm(formula = Richness~.,
              data = subset_mean,
              n.minobsinnode = hyper_grid$nodes[i],
              bag.fraction = hyper_grid$bag[i] ,
              interaction.depth =hyper_grid$idepth[i[]],
               n.trees = 10000,
               distribution = "gaussian",
               cv.folds =5)

oob_err[i] = model$train.error[length(model$train.error)]
  
 }

opt_i = which.min(oob_err)
print(hyper_grid[opt_i,])

```

```{r}
#OK, lets try that then
  
 model = gbm(formula = Richness~.,
              data = subset_mean,
              n.minobsinnode = 2,
              bag.fraction = 0.8 ,
              interaction.depth =6,
               n.trees = 10000,
               distribution = "gaussian",
               cv.folds =5)
print( model$train.error[length(model$train.error)])  


```

```{r}
pred <- predict(model,subset_mean,10000)
rmse(actual = subset_mean$Richness,
     predicted = pred)
```

```{r}
summary(model,n.trees=10000)

```

```{r}
#now need a train and test set

set.seed(5)
assignment <- sample(1:2, size = nrow(subset_mean), prob = c(0.75,0.25), replace = TRUE)

train <- subset_mean[assignment == 1, ]   
test <- subset_mean[assignment == 2, ]  

```


```{r}
nodes  = seq(2,10,1)
idepth = seq(2,6,1)
bag = seq(0.5,0.8,0.1)
hyper_grid = expand.grid(nodes = nodes, bag = bag, idepth = idepth)

oob_err = c()

for (i in 1:nrow(hyper_grid)){
 # browser()
  set.seed(1)
 model = gbm(formula = Richness~.,
              data = train,
              n.minobsinnode = hyper_grid$nodes[i],
              bag.fraction = hyper_grid$bag[i] ,
              interaction.depth =hyper_grid$idepth[i[]],
               n.trees = 10000,
               distribution = "gaussian",
               cv.folds =5)

oob_err[i] = model$train.error[length(model$train.error)]
  
 }

opt_i = which.min(oob_err)
print(hyper_grid[opt_i,])

```
Same hyper parameters on train set as when no train set used

```{r}
#run model on test set
model = gbm(formula = Richness~.,
              data = test,
              n.minobsinnode = 3,
              bag.fraction = 0.8 ,
              interaction.depth =6,
               n.trees = 10000,
               distribution = "gaussian",
               cv.folds =5)
print( model$train.error[length(model$train.error)])  


pred <- predict(model,test,10000)
rmse(actual = test$Richness,
     predicted = pred)
```

```{r}
s = summary(model,n.trees=10000)

x = as.character(s$var)
y = s$rel.inf
data = as.data.frame(cbind(x,y))
data$y = as.numeric(levels(data$y))[data$y]
data$x <- factor(data$x, levels = data$x[order(data$y)])
ggplot(data =data , aes(x = x, y = y)) +  geom_bar(stat = "identity")+
  ylab("relative influence")+
  xlab("")+
  theme(axis.text.x=element_text(angle = 45, hjust = 1))
```

```{r}
#now need a train and test set

set.seed(5)
assignment <- sample(1:2, size = nrow(subset_sd), prob = c(0.75,0.25), replace = TRUE)

train <- subset_sd[assignment == 1, ]   
test <- subset_sd[assignment == 2, ]  

```


```{r}
nodes  = seq(2,10,1)
idepth = seq(2,6,1)
bag = seq(0.5,0.8,0.1)
hyper_grid = expand.grid(nodes = nodes, bag = bag, idepth = idepth)

oob_err = c()

for (i in 1:nrow(hyper_grid)){
 # browser()
  set.seed(1)
 model = gbm(formula = Richness~.,
              data = train,
              n.minobsinnode = hyper_grid$nodes[i],
              bag.fraction = hyper_grid$bag[i] ,
              interaction.depth =hyper_grid$idepth[i[]],
               n.trees = 10000,
               distribution = "gaussian",
               cv.folds =5)

oob_err[i] = model$train.error[length(model$train.error)]
  
 }

opt_i = which.min(oob_err)
print(hyper_grid[opt_i,])

```
Same hyper parameters on train set as when no train set used
Gives nodes = 2, bag = 0.8, depth = 6
```{r}
#run model on test set
model = gbm(formula = Richness~.,
              data = test,
              n.minobsinnode = 3 ,
              bag.fraction = 0.8 ,
              interaction.depth =6,
               n.trees = 10000,
               distribution = "gaussian",
               cv.folds =5)
print( model$train.error[length(model$train.error)])  


pred <- predict(model,test,10000)
rmse(actual = test$Richness,
     predicted = pred)
```

```{r}
s = summary(model,n.trees=10000)

x = as.character(s$var)
y = s$rel.inf
data = as.data.frame(cbind(x,y))
data$y = as.numeric(levels(data$y))[data$y]
data$x <- factor(data$x, levels = data$x[order(data$y)])
ggplot(data =data , aes(x = x, y = y)) +  geom_bar(stat = "identity")+
  ylab("relative influence")+
  xlab("")+
  theme(axis.text.x=element_text(angle = 45, hjust = 1))

``` 