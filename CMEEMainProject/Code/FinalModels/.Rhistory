mean[[1]]
mean[[2]]
mean[[3]]
mean[[4]]
mean[[5]]
mean[[6]]
run_boosted_bootstrap = function(dataset){
dataset = dataset[,-1]
#browser()
# initialise
data = list()
# rmse_list = list()
#rmse_list[[1]] = 0
# rmse_list[[2]] = 0
#initalise a list for rel imp
rel_infl_list = list()
for (i in 1:ncol(dataset[-1])){
rel_infl_list[i] = 0
}
for (i in 1:100){
data = get_traintest(dataset)
model = get_model(data)
#rmse_train = get_rmse(model,data)[[1]]
#rmse_test = get_rmse(model,data)[[2]]
# rmse_list[[1]] = rmse_list[[1]] + rmse_train
#rmse_list[[2]] = rmse_list[[2]] + rmse_test
rel_infl = get_rel_infl(model)
rel_infl_ordered = get_influence(dataset,rel_infl)
rel_infl_list = add_list_elements(rel_infl_list,rel_infl_ordered)
}
variables = colnames(dataset[-1])
relinlf_vec = vector()
for (i in 1:12){
relinlf_vec[i] = as.numeric(rel_infl_list[[i]]/100)
}
#relinlf_df = data.frame(rel_influence = length(variables))
#for (i in 1:ncol(dataset[-1])){
# relinlf_df[i,1] = rel_infl_list[[i]]/100
#}
relinlf_df = as.data.frame(cbind(variables,relinlf_vec))
relinlf_df$relinlf_vec = as.double(relinlf_df$relinlf_vec)
#rmse_train_set = round(rmse_list[[1]]/100,2)
#rmse_test_set = round(rmse_list[[2]]/100,2)
#ggplot(data = relinlf_df , aes(x = reorder(variables, -relinlf_vec), y = relinlf_vec)) +
#geom_bar(stat = "identity")+
#ylab("relative influence")+
#xlab("")+
#labs(title = "MEAN dataset")+
#theme(axis.text.x=element_text(angle = 45, hjust = 1))+
#ggtitle("Relative Influence for 100 bootstrapped GBMs")+
#annotate("text", x = 10, y = 15, label = paste("rmse train set = ", rmse_train_set))+
#annotate("text", x = 10, y = 14, label = paste("rmse test set = ", rmse_test_set))+
#theme(text = element_text(size = 14, face = "bold"))
return(relinlf_df)
}
meanrelinfl = run_boosted_bootstrap(subset_mean)
View(meanrelinfl)
View(meanrelinfl)
?as.numeric
dataset = subset_mean
dataset = dataset[,-1] # remove site col
data = list()
rel_infl_list = list()
for (i in 1:ncol(dataset[-1])){
rel_infl_list[i] = 0
}
i = 1
data = get_traintest(dataset)
model = get_model(data)
rel_infl = get_rel_infl(model)
View(rel_infl)
rel_infl_ordered = get_influence(dataset,rel_infl)
rel_infl_list = add_list_elements(rel_infl_list,rel_infl_ordered)
variables = colnames(dataset[-1])
relinlf_vec = vector()
for (i in 1:12){
relinlf_vec[i] = round((rel_infl_list[[i]]/100),2)
}
relinlf_df = as.data.frame(cbind(variables,relinlf_vec))
relinlf_df$relinlf_vec = as.double(relinlf_df$relinlf_vec)
View(relinlf_df)
View(relinlf_df)
View(relinlf_df)
relinlf_df$relinlf_vec = (relinlf_df$relinlf_vec)
View(relinlf_df)
variables = colnames(dataset[-1])
relinlf_vec = vector()
for (i in 1:12){
relinlf_vec[i] = round((rel_infl_list[[i]]/100),2)
}
relinlf_df = as.data.frame(cbind(variables,relinlf_vec))
View(relinlf_df)
relinlf_df$relinlf_vec = (relinlf_df$relinlf_vec)
View(relinlf_df)
run_boosted_bootstrap = function(dataset){
dataset = dataset[,-1]
#browser()
# initialise
data = list()
# rmse_list = list()
#rmse_list[[1]] = 0
# rmse_list[[2]] = 0
#initalise a list for rel imp
rel_infl_list = list()
for (i in 1:ncol(dataset[-1])){
rel_infl_list[i] = 0
}
for (i in 1:100){
data = get_traintest(dataset)
model = get_model(data)
#rmse_train = get_rmse(model,data)[[1]]
#rmse_test = get_rmse(model,data)[[2]]
# rmse_list[[1]] = rmse_list[[1]] + rmse_train
#rmse_list[[2]] = rmse_list[[2]] + rmse_test
rel_infl = get_rel_infl(model)
rel_infl_ordered = get_influence(dataset,rel_infl)
rel_infl_list = add_list_elements(rel_infl_list,rel_infl_ordered)
}
variables = colnames(dataset[-1])
relinlf_vec = vector()
for (i in 1:12){
relinlf_vec[i] = round((rel_infl_list[[i]]/100),2)
}
#relinlf_df = data.frame(rel_influence = length(variables))
#for (i in 1:ncol(dataset[-1])){
# relinlf_df[i,1] = rel_infl_list[[i]]/100
#}
relinlf_df = as.data.frame(cbind(variables,relinlf_vec))
#relinlf_df$relinlf_vec = (relinlf_df$relinlf_vec)
#rmse_train_set = round(rmse_list[[1]]/100,2)
#rmse_test_set = round(rmse_list[[2]]/100,2)
#ggplot(data = relinlf_df , aes(x = reorder(variables, -relinlf_vec), y = relinlf_vec)) +
#geom_bar(stat = "identity")+
#ylab("relative influence")+
#xlab("")+
#labs(title = "MEAN dataset")+
#theme(axis.text.x=element_text(angle = 45, hjust = 1))+
#ggtitle("Relative Influence for 100 bootstrapped GBMs")+
#annotate("text", x = 10, y = 15, label = paste("rmse train set = ", rmse_train_set))+
#annotate("text", x = 10, y = 14, label = paste("rmse test set = ", rmse_test_set))+
#theme(text = element_text(size = 14, face = "bold"))
return(relinlf_df)
}
meanrelinfl = run_boosted_bootstrap(subset_mean)
View(meanrelinfl)
View(meanrelinfl)
order(meanrealinfl,relinlf_vec, desc = TRUE)
order(meanrelinfl, relinlf_vec, desc = TRUE)
?order
ordered_relinfl = meanrelinfl[order(relinlf_vec),]
View(ordered_relinfl)
View(ordered_relinfl)
## NB - aseveral libraries mask each other here - arm masks dplyr and corrplot, therefore open libraries #as required.
knitr::opts_chunk$set(
echo = FALSE,
message = FALSE,
warning = FALSE
)
# Richness modelling
rm(list = ls())
cat("\014")
library(dplyr) # everything
library(ggplot2)
library(corrplot)
library(car) # for vif
library(reshape) # melt
zeta_r = readRDS("../Zeta/zeta_r")
# get the data
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
site_data = site_data[,-1]
Site = c(1:103)
zeta_r = as.data.frame(cbind(Site,zeta_r))
site_data = inner_join(site_data,zeta_r)
#mean impute the missing PHI
meanPHI = round(mean(site_data$Pos_Hetero_Index, na.rm = TRUE),2)
x = site_data$Pos_Hetero_Index
x[is.na(x)] = meanPHI
site_data$Pos_Hetero_Index = x
# slect only required variables
subset_all = site_data%>%select(Site,Richness,Area_ha,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
colnames(subset_all) = c("Site","Richness","Area",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meanTD","zeta_r")
#remove the wood with the largest area
largest_area = as.numeric(subset_all%>%filter(Area == max(Area))%>%select(Site))
site_data_outlier = subset_all%>%filter(Site!=largest_area)
site_data_outlier = site_data_outlier[,-3] # remove area column now
subset_sd = site_data_outlier%>%select(Site,Richness,
PHI,Buffer,Northing,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_TD,area_ratio,zeta_r)
#,
subset_mean = site_data_outlier%>%select(Site,Richness,
PHI, Northing, meandbh,Buffer,
meanph,  meanSOM,meanTD,meanLBA, area_ratio,no_NVC,
no_MSG,zeta_r)
#,
#look at correaltions between explanatory variables
vars = site_data_outlier[,-c(1,2)]
mcor = round(cor(vars, method = "pearson", use = "na.or.complete"),2)
corrplot(mcor, type = "upper", tl.pos = "td",
method = "number", tl.cex = 0.5, tl.col = 'black',tl.srt=45,
order = "hclust", diag = FALSE,
title = "pearson correlation",
mar=c(0,0,1,0))
mcor = round(cor(vars, method = "spearman", use = "na.or.complete"),2)
corrplot(mcor, type = "upper", tl.pos = "td",
method = "number", tl.cex = 0.5, tl.col = 'black',tl.srt=45,
order = "hclust", diag = FALSE,
title = "spearman correlation",
mar=c(0,0,1,0))
corrplot(mcor, type = "upper", tl.pos = "td",
method = "number", tl.cex = 0.5, tl.col = 'black',tl.srt=45,
order = "hclust", diag = FALSE,
title = "spearman correlation",
mar=c(0,0,1,0))
###Means correlations...
library(corrplot)
vars = subset_mean[,-c(1,2)]
mcor = round(cor(vars, method = "pearson", use = "na.or.complete"),2)
corrplot(mcor, type = "upper", tl.pos = "td",
method = "number", tl.cex = 1, tl.col = 'black',tl.srt=45,
order = "hclust", diag = FALSE,
title = "mean dataset pearson correlation",
number.cex = 1,
mar=c(0,0,1,0))
mcor = round(cor(vars, method = "spearman", use = "na.or.complete"),2)
corrplot(mcor, type = "upper", tl.pos = "td",
method = "number", tl.cex = 1, tl.col = 'black',tl.srt=45,
order = "hclust", diag = FALSE,
title = "mean dataset spearman correlation",
number.cex = 1,
mar=c(0,0,1,0))
##Models for means#############
# we know richness vs ph usually unimodal around .5, therefore fit to meanpH^2
data = subset_mean[,-1]
richness = subset_mean[,2] # removed because going to scale in next chunk
#data$meanph2 = (data$meanph)^2 #just leave as meanpH because doesnt come out either way??
#dont do this because meanpH and meanpH2 correlated and messes it all up. Just accept that model wont select mean pH very well
#rescale the mean data
library(arm) #for standarize
rescaled_mean_data = apply(data[,-1],2, rescale)
rescaled_mean_data = as.data.frame(cbind(richness, rescaled_mean_data))
#create the model
mod_mean = lm(richness~., data=rescaled_mean_data, na.action = "na.fail")
#have a look at the linear model
par(mfrow =c(2,2))
plot(mod_mean)
# look at vifs
vif(mod_mean)
library(MuMIn) #dredge and avg
#get top models
models = dredge(mod_mean)
model_set = get.models(models, subset = delta<2)
#do model averaging, subset means zero method
mean_avg_models = model.avg(model_set, subset)
#select output data
summary = summary(mean_avg_models)
coefs =  mean_avg_models$coefficients
importance =  c(NA,(as.vector(mean_avg_models$importance)))
coef_matrix = summary$coefmat.full
coefs = coef_matrix[,1]
adj_se = coef_matrix[,3]
CI_lower =  coefs - 1.96*adj_se
CI_upper = coefs + 1.96*adj_se
output = round(as.data.frame(cbind(coefs,adj_se, CI_lower, CI_upper, importance)),2)
# make plot of the variables and CI
n = nrow(output)
data = output[c(2:n),]
data = data[,c(1,3,4)]
data = t(data)
data = as.data.frame(data)
melted = melt(data)
fun_mean <- function(x){
data = data.frame(y=mean(x),label=mean(x,na.rm=T))
data = round(data,2)
return(data)}
#(aes_string(x = 'variable', y='value', na.rm = TRUE)
melted$variable = as.factor(melted$variable)# ps, wont plot as separate plots if x continous
ggplot(melted,aes_string(x = 'variable', y ='value' ) )+
geom_boxplot(na.rm = TRUE, width = 0)+
stat_summary(fun.y = mean, geom="point",colour="darkred", size=3) +
stat_summary(fun.data = fun_mean, geom="text", vjust=-0.7)+
stat_summary(geom = "text", label = output$importance[-1],fun.y = max, hjust = 1, colour = "red")+
geom_abline(intercept = 0, slope = 0)+
labs(y = "Effect size and 95%CI",x = "Parameter")+
#labs(title = "MEAN dataset")+
theme(axis.text.x = element_text(angle=45, hjust=1))+
theme(text = element_text(size = 14, face = "bold"))
## NB - aseveral libraries mask each other here - arm masks dplyr and corrplot, therefore open libraries #as required.
knitr::opts_chunk$set(
echo = FALSE,
message = FALSE,
warning = FALSE
)
# Richness modelling
rm(list = ls())
cat("\014")
library(dplyr) # everything
library(ggplot2)
library(car) # for vif
library(reshape) # melt
# get the data
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
nestZs = readRDS("../nest_mixed_model_fits.RDS")
Zs = nestZs%>%select(Site,slope)
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
nestZs = readRDS("../nest_mixed_model_fits.RDS")
View(nestZs)
View(nestZs)
Zs = nestZs%>%select(Site,slope)
## NB - several libraries mask each other here - arm masks dplyr and corrplot, therefore open libraries #as required.
knitr::opts_chunk$set(
echo = FALSE,
message = FALSE,
warning = FALSE
)
# Richness modelling
rm(list = ls())
cat("\014")
library(dplyr) # everything
library(ggplot2)
library(car) # for vif
library(reshape) # melt
# get the data
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
PlotZdata = read.csv("../../Data/z_ave_fits.csv") # these from my min/max method
Zs = PlotZdata%>%select(Site, slope)
zeta_r = readRDS("../Zeta/zeta_r")
Site = c(1:103)
zeta_r = as.data.frame(cbind(Site,zeta_r))
site_data = inner_join(site_data,zeta_r)
#mean impute the missing PHI
meanPHI = round(mean(site_data$Pos_Hetero_Index, na.rm = TRUE),2)
x = site_data$Pos_Hetero_Index
x[is.na(x)] = meanPHI
site_data$Pos_Hetero_Index = x
site_data_zs = inner_join(site_data,Zs)#now has my min/mx slopes
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_zs_vegan%>%select(Site,veganrandzs,Area_ha,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
View(site_data_zs)
View(site_data_zs)
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_zs_vegan%>%select(Site,slope,Area_ha,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_zs%>%select(Site,slope,Area_ha,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
#zeta_r"
colnames(subset_all) = c("Site","z","Area",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meanTD","zeta_r")
#,"zeta_r"
View(PlotZdata)
View(PlotZdata)
View(site_data_zs)
View(site_data_zs)
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_zs%>%select(Site,slope,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
#zeta_r"
colnames(subset_all) = c("Site","z",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meanTD","zeta_r")
#,"zeta_r"
#remove the wood with the largest area
largest_area = as.numeric(subset_all%>%filter(Area == max(Area))%>%select(Site))
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_zs%>%select(Site,slope,Area
Northing, Pos_Hetero_Index,Buffer3,
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_zs%>%select(Site,slope,Area,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_zs%>%select(Site,slope,Area_ha,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
#zeta_r"
colnames(subset_all) = c("Site","z","Area"
"Northing", "PHI","Buffer",
# select only required variables
#change to either slope or zr or zexp if you want minmax , 1000 random paths or expectation
subset_all = site_data_zs%>%select(Site,slope,Area_ha,
Northing, Pos_Hetero_Index,Buffer3,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_treedensity,area_ratio,
meandbh,meanph, meanSOM,meanLBA,
meantreedensity,zeta_r)
#zeta_r"
colnames(subset_all) = c("Site","z","Area",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meanTD","zeta_r")
#,"zeta_r"
#remove the wood with the largest area
largest_area = as.numeric(subset_all%>%filter(Area == max(Area))%>%select(Site))
site_data_outlier1 = subset_all%>%filter(Site!=largest_area)
site_data_outlier1 = site_data_outlier1[,-3] # remove area column now
#remove the outlier in PHI
largest_PHI = as.numeric(subset_all%>%filter(PHI == max(PHI))%>%select(Site))
site_data_outlier2 = site_data_outlier1%>%filter(Site!=largest_PHI)
largest_PHI = as.numeric(site_data_outlier2%>%filter(PHI == max(PHI))%>%select(Site))
site_data_outlier3 = site_data_outlier2%>%filter(Site!=largest_PHI)
subset_sd = site_data_outlier3%>%select(Site,z,
Northing, PHI,Buffer,
no_MSG, no_NVC,sd_pH,sd_SOM,sd_LBA,
sd_meandbh,sd_TD,area_ratio,zeta_r)
#,"zeta_r"
subset_mean = site_data_outlier3%>%select(Site,z,
Northing, PHI,meandbh,
meanph, Buffer, meanSOM,meanLBA,
meanTD,area_ratio, no_NVC,
no_MSG,zeta_r)
#,"zeta_r"
# we know richness vs ph usually unimodal around .5, therefore fit to meanpH^2
data = subset_mean[,-1]
z = subset_mean[,2]
#data$meanph = (data$meanph)^2
#rescale the data
library(arm) #for standarize
rescaled_mean_data = apply(data[,-1],2, rescale)
rescaled_mean_data = as.data.frame(cbind(z, rescaled_mean_data))
#create the model
mod_mean = lm(z~., data=rescaled_mean_data, na.action = "na.fail")
#have a look at the linear model
par(mfrow =c(2,2))
plot(mod_mean, main = "Mean dataset")
library(MuMIn) #dredge and avg
#get top models
models = dredge(mod_mean)
model_set = get.models(models, subset = delta<0.8)
#do model averaging, subset means zero method
mean_avg_models = model.avg(model_set, subset)
#select output data
summary = summary(mean_avg_models)
coefs =  mean_avg_models$coefficients
importance =  c(NA,(as.vector(mean_avg_models$importance)))
coef_matrix = summary$coefmat.full
coefs = coef_matrix[,1]
adj_se = coef_matrix[,3]
CI_lower =  coefs - 1.96*adj_se
CI_upper = coefs + 1.96*adj_se
output = round(as.data.frame(cbind(coefs,adj_se, CI_lower, CI_upper, importance)),2)
# make plot of the variables and CI
n = nrow(output)
data = output[c(2:n),]
data = data[,c(1,3,4)]
data = t(data)
data = as.data.frame(data)
melted = melt(data)
fun_mean <- function(x){
data = data.frame(y=mean(x),label=mean(x,na.rm=T))
data = round(data,2)
return(data)}
#(aes_string(x = 'variable', y='value', na.rm = TRUE)
melted$variable = as.factor(melted$variable)# ps, wont plot as separate plots if x continous
ggplot(melted,aes_string(x = 'variable', y ='value' ) )+
geom_boxplot(na.rm = TRUE, width = 0)+
stat_summary(fun.y = mean, geom="point",colour="darkred", size=3) +
stat_summary(fun.data = fun_mean, geom="text", vjust=-0.7)+
stat_summary(geom = "text", label = output$importance[-1],fun.y = max, hjust = 1, colour = "red")+
geom_abline(intercept = 0, slope = 0)+
labs(y = "Effect size and 95%CI",x = "effect")+
labs(title = "Model averaged results for delta <1.5, Plot Zs, Mean dataset",
subtitle = "numbers in red are variable importance")
