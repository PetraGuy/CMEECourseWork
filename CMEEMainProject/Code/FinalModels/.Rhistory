rich_model = bagging(formula = Richness~.,
data = rich_train,
coob = TRUE)
install.packages("ipred")
#bagged trees
library(ipred)
set.seed(123)
rich_model = bagging(formula = Richness~.,
data = rich_train,
coob = TRUE)
print(rich_model)
knitr::opts_chunk$set(echo = FALSE)
rm(list = ls())
cat("\014")
library(rpart)
library(rpart.plot)
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
site_data = site_data[,-1]
# select required variables
subset_all = site_data%>%select("Site","Richness","Area_ha",
"Northing", "Pos_Hetero_Index","Buffer3",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_treedensity","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meantreedensity")
colnames(subset_all) = c("Site","Richness","Area",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meanTD")
#remove the wood with the largest area
largest_area = as.numeric(subset_all%>%filter(Area == max(Area))%>%select(Site))
site_data_outlier = subset_all%>%filter(Site!=largest_area)
site_data_outlier = site_data_outlier[,-3] # remove area column now
subset_sd = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio")
subset_mean = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI",  "meandbh",
"meanph", "Buffer", "meanSOM","meanLBA",
"meanTD","area_ratio", "no_NVC",
"no_MSG")
#make 3 datasets, test, train and validate
set.seed(1)
assignment <- sample(1:3, size = nrow(subset_mean), prob = c(0.7,0.15,0.15), replace = TRUE)
# Create a train, validation and tests from the original data frame
rich_train <- subset_mean[assignment == 1, ]    # subset the grade data frame to training indices only
rich_valid <- subset_mean[assignment == 2, ]  # subset the grade data frame to validation indices only
rich_test <- subset_mean[assignment == 3, ]   # subset the grade data frame to test indices only
#look at the model
rich_model <- rpart(formula = Richness ~ .,
data = rich_train,
method = "anova")
# Look at the model output
print(rich_model)
# Plot the tree model
rpart.plot(x = rich_model, yesno = 2, type = 0, extra = 0)
library(Metrics)
pred = predict(object = rich_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
plotcp(rich_model)
library(Metrics)
pred = predict(object = rich_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
plotcp(rich_model)
print(rich_model$cptable)
opt_index = which.min(rich_model$cptable[,"xerror"])
opt_index = which.min(rich_model$cptable[,"xerror"])
cp_opt = rich_model$cptable[opt_index,"CP"]
model_opt1 = prune(tree = rich_model, cp = cp_opt)
pred = predict(object = model_opt1,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
rich_model$variable.importance
View(subset_all)
View(subset_mean)
subset_sd = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio")
subset_sd = subset_sd[,-1] # remove site column
subset_mean = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI",  "meandbh",
"meanph", "Buffer", "meanSOM","meanLBA",
"meanTD","area_ratio", "no_NVC",
"no_MSG")
subset_sd = subset_sd[,-1] # remove site column
#make 3 datasets, test, train and validate
set.seed(1)
assignment <- sample(1:3, size = nrow(subset_mean), prob = c(0.7,0.15,0.15), replace = TRUE)
# Create a train, validation and tests from the original data frame
rich_train <- subset_mean[assignment == 1, ]    # subset the grade data frame to training indices only
rich_valid <- subset_mean[assignment == 2, ]  # subset the grade data frame to validation indices only
rich_test <- subset_mean[assignment == 3, ]   # subset the grade data frame to test indices only
#look at the model
rich_model <- rpart(formula = Richness ~ .,
data = rich_train,
method = "anova")
# Look at the model output
print(rich_model)
# Plot the tree model
rpart.plot(x = rich_model, yesno = 2, type = 0, extra = 0)
library(Metrics)
pred = predict(object = rich_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
plotcp(rich_model)
rich_model$variable.importance
View(subset_mean)
View(subset_mean)
subset_sd = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio")
subset_sd = subset_sd[,-1] # remove site column
subset_mean = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI",  "meandbh",
"meanph", "Buffer", "meanSOM","meanLBA",
"meanTD","area_ratio", "no_NVC",
"no_MSG")
subset_sd = subset_sd[,-1] # remove site column
View(subset_mean)
subset_sd = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio")
subset_sd = subset_sd[,-1] # remove site column
subset_mean = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI",  "meandbh",
"meanph", "Buffer", "meanSOM","meanLBA",
"meanTD","area_ratio", "no_NVC",
"no_MSG")
subset_sd = subset_sd[,-1] # remove site column
View(subset_sd)
View(subset_mean)
subset_sd = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio")
subset_sd = subset_sd[,-1] # remove site column
subset_mean = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI",  "meandbh",
"meanph", "Buffer", "meanSOM","meanLBA",
"meanTD","area_ratio", "no_NVC",
"no_MSG")
subset_mean = subset_mean[,-1] # remove site column
#make 3 datasets, test, train and validate
set.seed(1)
assignment <- sample(1:3, size = nrow(subset_mean), prob = c(0.7,0.15,0.15), replace = TRUE)
# Create a train, validation and tests from the original data frame
rich_train <- subset_mean[assignment == 1, ]    # subset the grade data frame to training indices only
rich_valid <- subset_mean[assignment == 2, ]  # subset the grade data frame to validation indices only
rich_test <- subset_mean[assignment == 3, ]   # subset the grade data frame to test indices only
#look at the model
rich_model <- rpart(formula = Richness ~ .,
data = rich_train,
method = "anova")
# Look at the model output
print(rich_model)
# Plot the tree model
rpart.plot(x = rich_model, yesno = 2, type = 0, extra = 0)
library(Metrics)
pred = predict(object = rich_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
plotcp(rich_model)
print(rich_model$cptable)
rich_model$variable.importance
library(Metrics)
pred = predict(object = rich_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
plotcp(rich_model)
plotcp(rich_model)
opt_index = which.min(rich_model$cptable[,"xerror"])
cp_opt = rich_model$cptable[opt_index,"CP"]
model_opt1 = prune(tree = rich_model, cp = cp_opt)
pred = predict(object = model_opt1,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
opt_index = which.min(rich_model$cptable[,"xerror"])
cp_opt = rich_model$cptable[opt_index,"CP"]
model_opt1 = prune(tree = rich_model, cp = cp_opt)
pred = predict(object = model_opt1,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
minsplit = seq(2,4,6,8,10)
?seq
minsplit = seq(2,10,2)
maxdepth = seq(1,11,1)
hyper_grid = expand.grid(minsplit = minsplit, maxdepth = maxdepth)
#create a hyperparameter tuning grid
#first look at split parameter
minsplit = seq(2,10,2)
maxdepth = seq(1,11,1)
hyper_grid = expand.grid(minsplit = minsplit, maxdepth = maxdepth)
num_models = nrow(hyper_grid)
rich_models = list()
for (i in 1:num_models){
minsplit = hyper_grid$minsplit[i]
maxdepth = hyper_grid$maxdepth[i]
rich_models[[i]] = rpart(formula = Richness ~ .,
data = rich_train,
method = "anova",
minsplit = minsplit,
maxdepth = maxdepth)
}
rmses = c()
for (i in 1:num_models){
model = rich_models[[i]]
pred = predict(object = model,
newdata = rich_valid)
rmses[i] = rmse(actual = rich_valid$Richness,
predicted = pred)
}
best_model = rich_models[[which.min(rmses)]]
best_model$control
pred = predict(object = best_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
#bagged trees
library(ipred)
set.seed(123)
rich_model = bagging(formula = Richness~.,
data = rich_train,
coob = TRUE)
print(rich_model)
pred = predict(object = rich_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
install.packages("caret")
installed.packages()
?trainControl
??trainControl
# redo this with K fold cross validation for model performance metricss
library(caret)
ctrl = trainControl(method = "cv",
number = 5,
classProbs = FALSE,
summary = "RMSE")
set.seed(1)
rich_model = train(Richness~.,
data = subset_mean)
rich_model = train(Richness~.,
data = subset_mean,
na.action = na.omit)
library(randomForest)
rich_model = train(Richness~.,
data = subset_mean,
na.action = na.omit)
rich_model = train(Richness~.,
data = rich_train,
na.action = na.omit)
rich_model = train(Richness~.,
data = rich_train,
method = "treebag",
metric = "RMSE",
trControl = ctrl
na.action = na.omit)
rich_model = train(Richness~.,
data = rich_train,
method = "treebag",
metric = "RMSE",
trControl = ctrl,
na.action = na.omit)
ctrl = trainControl(method = "cv",
number = 5,
classProbs = FALSE,
summary = "RMSE")
?trainControl
ctrl = trainControl(method = "cv",
number = 5,
classProbs = FALSE)
?method
# redo this with K fold cross validation for model performance metricss
library(caret)
library(randomForest)
ctrl = trainControl(method = "cv",
number = 5,
classProbs = FALSE)
set.seed(1)
rich_model = train(Richness~.,
data = rich_train,
method = "treebag",
metric = "RMSE",
trControl = ctrl,
na.action = na.omit)
pred = predict(rich_model, rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
length(pred)
lenth(rich_test)
length(rich_test)
rich_test$Richness
pred
length(pred)
length(rich_test$Richness)
# redo this with K fold cross validation for model performance metricss
library(caret)
library(randomForest)
ctrl = trainControl(method = "cv",
number = 5,
classProbs = FALSE)
set.seed(1)
rich_model = train(Richness~.,
data = rich_train,
method = "adaboost",
metric = "RMSE",
trControl = ctrl,
na.action = na.omit)
# redo this with K fold cross validation for model performance metricss
library(caret)
library(randomForest)
ctrl = trainControl(method = "cv",
number = 5,
classProbs = FALSE)
set.seed(1)
rich_model = train(Richness~.,
data = rich_train,
method = "bayesglm",
metric = "RMSE",
trControl = ctrl,
na.action = na.omit)
pred = predict(rich_model, rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
length(pred)
rich_model = train(Richness~.,
data = rich_train,
method = "cforest",
metric = "RMSE",
trControl = ctrl,
na.action = na.omit)
length(rich_test$Richness)
length(pred)
pred = predict(rich_model, rich_test)
pred
rich_test$Richness
pred = predict(object = rich_model,
newdata = rich_test)
length(precip)
length(pred)
rich_test
meanPHI = round(mean(site_data$Pos_Hetero_Index, na.rm = TRUE),2)
x = site_data$Pos_Hetero_Index
x[is.na(x)] = meanPHI
site_data$Pos_Hetero_Index = x
subset_all = site_data%>%select("Site","Richness","Area_ha",
"Northing", "Pos_Hetero_Index","Buffer3",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_treedensity","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meantreedensity")
colnames(subset_all) = c("Site","Richness","Area",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meanTD")
#remove the wood with the largest area
largest_area = as.numeric(subset_all%>%filter(Area == max(Area))%>%select(Site))
site_data_outlier = subset_all%>%filter(Site!=largest_area)
site_data_outlier = site_data_outlier[,-3] # remove area column now
subset_sd = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI","Buffer",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_TD","area_ratio")
subset_sd = subset_sd[,-1] # remove site column
subset_mean = site_data_outlier%>%select("Site","Richness",
"Northing", "PHI",  "meandbh",
"meanph", "Buffer", "meanSOM","meanLBA",
"meanTD","area_ratio", "no_NVC",
"no_MSG")
subset_mean = subset_mean[,-1] # remove site column
#make 3 datasets, test, train and validate
set.seed(1)
assignment <- sample(1:3, size = nrow(subset_mean), prob = c(0.7,0.15,0.15), replace = TRUE)
# Create a train, validation and tests from the original data frame
rich_train <- subset_mean[assignment == 1, ]    # subset the grade data frame to training indices only
rich_valid <- subset_mean[assignment == 2, ]  # subset the grade data frame to validation indices only
rich_test <- subset_mean[assignment == 3, ]   # subset the grade data frame to test indices only
#look at the model
rich_model <- rpart(formula = Richness ~ .,
data = rich_train,
method = "anova")
# Look at the model output
print(rich_model)
# Plot the tree model
rpart.plot(x = rich_model, yesno = 2, type = 0, extra = 0)
library(Metrics)
pred = predict(object = rich_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
plotcp(rich_model)
print(rich_model$cptable)
opt_index = which.min(rich_model$cptable[,"xerror"])
cp_opt = rich_model$cptable[opt_index,"CP"]
model_opt1 = prune(tree = rich_model, cp = cp_opt)
pred = predict(object = model_opt1,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
#create a hyperparameter tuning grid
#first look at split parameter
minsplit = seq(2,10,2)
maxdepth = seq(1,11,1)
hyper_grid = expand.grid(minsplit = minsplit, maxdepth = maxdepth)
num_models = nrow(hyper_grid)
rich_models = list()
for (i in 1:num_models){
minsplit = hyper_grid$minsplit[i]
maxdepth = hyper_grid$maxdepth[i]
rich_models[[i]] = rpart(formula = Richness ~ .,
data = rich_train,
method = "anova",
minsplit = minsplit,
maxdepth = maxdepth)
}
rmses = c()
for (i in 1:num_models){
model = rich_models[[i]]
pred = predict(object = model,
newdata = rich_valid)
rmses[i] = rmse(actual = rich_valid$Richness,
predicted = pred)
}
best_model = rich_models[[which.min(rmses)]]
best_model$control
pred = predict(object = best_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
#bagged trees
library(ipred)
set.seed(123)
rich_model = bagging(formula = Richness~.,
data = rich_train,
coob = TRUE)
print(rich_model)
pred = predict(object = rich_model,
newdata = rich_test)
rmse(actual = rich_test$Richness,
predicted = pred)
# redo this with K fold cross validation for model performance metricss
library(caret)
library(randomForest)
ctrl = trainControl(method = "cv",
number = 5,
classProbs = FALSE)
set.seed(1)
rich_model = train(Richness~.,
data = rich_train,
method = "cforest",
metric = "RMSE",
trControl = ctrl,
na.action = na.omit)
rich_model = train(Richness~.,
data = rich_train,
method = "cforest",
metric = "RMSE",
trControl = ctrl)
knitr::opts_chunk$set(echo = FALSE)
rm(list = ls())
cat("\014")
library(rpart)
library(rpart.plot)
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
site_data = site_data[,-1]
#mean impute the missing PHI
meanPHI = round(mean(site_data$Pos_Hetero_Index, na.rm = TRUE),2)
x = site_data$Pos_Hetero_Index
x[is.na(x)] = meanPHI
site_data$Pos_Hetero_Index = x
subset_all = site_data%>%select("Site","Richness","Area_ha",
"Northing", "Pos_Hetero_Index","Buffer3",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_treedensity","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meantreedensity")
knitr::opts_chunk$set(echo = FALSE)
rm(list = ls())
cat("\014")
library(rpart)
library(rpart.plot)
site_data =  read.csv("../../Data/CompleteSiteLevelVars.csv")
site_data = site_data[,-1]
#mean impute the missing PHI
meanPHI = round(mean(site_data$Pos_Hetero_Index, na.rm = TRUE),2)
x = site_data$Pos_Hetero_Index
x[is.na(x)] = meanPHI
site_data$Pos_Hetero_Index = x
subset_all = site_data%>%select("Site","Richness","Area_ha",
"Northing", "Pos_Hetero_Index","Buffer3",
"no_MSG", "no_NVC","sd_pH","sd_SOM","sd_LBA",
"sd_meandbh","sd_treedensity","area_ratio",
"meandbh","meanph", "meanSOM","meanLBA",
"meantreedensity")
