colnames(m1_percent) = c(1:10)
kable(m1_percent, format = "latex", caption = "Percentage of true positives for non-standarized data")%>%
kable_styling(latex_options = "hold_position")
m2_percent = round(apply(m2, 1, function(x) (x/numspecies)*100), digits = 2)
m3_percent = round(apply(m3, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m3_percent) = c(1:10)
kable(m3_percent, format = "latex", caption = "Percentage of true positives for standarized data")%>%
kable_styling(latex_options = "hold_position")
prec_df_Imputed = Imputed_kmeans[[4]]
colnames(prec_df_Imputed) = c(1:10)
kable(prec_df_Imputed, format = "latex", caption = "Precision of kmeans with non standardized data")%>%
kable_styling(latex_options = "hold_position")
prec_df = Scaled_kmeans[[4]]
colnames(prec_df) = c(1:10)
kable(prec_df, format = "latex", caption = "Precision of kmeans with standardized data")%>%
kable_styling(latex_options = "hold_position")
sens_df_Imputed = Imputed_kmeans[[4]]
colnames(sens_df_Imputed) = c(1:10)
kable(sens_df_Imputed, format = "latex", caption = "Sensitivity of kmeans with non standardized data")%>%
kable_styling(latex_options = "hold_position")
knitr::opts_chunk$set(echo = FALSE,message = FALSE,fig.pos = "H" ,comment=NA, fig.align ="centre")
#clear the workspace
rm(list = ls())
cat("\014")
#setwd("~/Documents/CMEECourseWork/MiniProject/Code")
library(ggplot2)
library(reshape) # both required for the box plots, otherwise they cant all be presented
# on one page and therefore difficult to analyse
library(rpart)
library(rpart.plot)# both required for the decision tree
library(knitr)
library(kableExtra) # for kable stylig options, to hold position on page
library(rmarkdown)
knitr::opts_chunk$set(echo = FALSE)
rm(list = ls())
cat("\014")
library(dplyr)
library(ggplot2)
library(ggmap)
library(ggrepel)
library(nlme)
library(reshape)
library(gridExtra)
#the wood location data
woods = read.csv("../Data/EastingNorthing.csv")
colnames(woods) = c("Site", "Easting","Northing","GridRef","Lat","Long")
#just for plotting
data = cbind(woods$Lat, woods$Long)
#Input all the ground flora data
Data = read.csv("../Data/GroundCover.csv")
Data_Yr2 = Data%>%filter(Yr_2 == 2)#%>%select(SITE,PLOT,NEST,COV,Amalgams)
colnames(Data_Yr2) = c("SITE", "PLOT","NEST","Cover","BRC_number","Year")
veg_codes = read.csv("../Data/vegetation_codes.csv")
# the  bryophytes, lichen etc have already been removed from these.
colnames(veg_codes) = c("Species", "BRC_number")
Data_Yr2_veg = Data_Yr2%>% inner_join(veg_codes)
# returns the species richness for each site/plot/nest
# NAs are not counted, these occur for sapling counts which need to be added in to the plot richness.
nest_richness = function(data){
site_list = list()
#browser()
for (i in 1:103){
sitedata = data%>%filter(SITE == i)
plot_nest_df = data.frame()
for (j in 1:16){
plotdata = sitedata%>%filter(PLOT == j)
tmp = vector()
for (k in 1:5){
nestdata = plotdata%>%filter(NEST ==k)
tmp = c(tmp, nrow(nestdata))
}
plot_nest_df = rbind(plot_nest_df, tmp)
colnames(plot_nest_df) = c("nest1","nest2","nest3","nest4","nest5")
}
site_list[[i]]= plot_nest_df
}
return(site_list)
}
spec_rich = nest_richness(Data_Yr2_veg)
# now need to include the NAs and combine them into a plot richness
plot_richness = function(data){
site_matrix = matrix(nrow = 103, ncol = 16)
#browser()
for (i in 1:103){
sitedata = data%>%filter(SITE == i)
plot_nest_df = data.frame()
for (j in 1:16){
plotdata = sitedata%>%filter(PLOT == j)
site_matrix[i,j] = length(unique(plotdata$BRC_number))
}
}
return(site_matrix)
}
plot_rich = plot_richness(Data_Yr2_veg)
write.csv(plot_rich,"../Data/plot_rich.csv")
# now need to include the NAs and combine them into a plot richness
plot_richness = function(data){
site_matrix = matrix(nrow = 103, ncol = 16)
#browser()
for (i in 1:103){
sitedata = data%>%filter(SITE == i)
plot_nest_df = data.frame()
for (j in 1:16){
plotdata = sitedata%>%filter(PLOT == j)
site_matrix[i,j] = length(unique(plotdata$BRC_number))
}
}
return(site_matrix)
}
plot_rich = plot_richness(Data_Yr2_veg)
write.csv(plot_rich,"../Data/plot_rich.csv")
# just calculates richness found in each wood, no extrapolation.
#d[i] is richness of wood i
basic_richness = function(data){
d = rep(0,103)
for (i in 1:103){
site = data%>%filter(SITE==i)
d[i]  = length(unique(site$BRC_number))
}
return(d)
}
d = basic_richness(Data_Yr2_veg)
## cumulative richness for site 1, for all 16 plots.
##need spec_rich calculated above
cum_rich_all = list()
for ( i in 1:103){
#browser()
site = spec_rich[[i]]
site = as.data.frame(site)
cum_rich_site = data.frame()
for (j in 1:16) {
plot = site[j,]
r = 0
cumrich = rep(0,5)
for (n in 1:5){
r = r + plot[[n]]
cumrich[n] = r
}
cum_rich_site = rbind(cum_rich_site, cumrich)
}
colnames(cum_rich_site) = c("nest1","nest2","nest3","nest4","nest5")
cum_rich_all[[i]] = cum_rich_site
}
#some stats, might use these
wood_rich = cbind(woods,d) # just add richness to loactions
max_wood = wood_rich%>%filter(Site==match(max(d),d))
min_wood = wood_rich%>%filter(Site==match(min(d),d))
#wont work with 0s
plot_rich[plot_rich == 0] = NA
min_max = rbind(min_wood, max_wood)
mins = apply(plot_rich,1,min,na.rm=TRUE)
maxs = apply(plot_rich,1,max,na.rm=TRUE)
ranges = maxs - mins
max_range= max(ranges)
min_range = min(ranges)
max_range_wood = wood_rich%>%filter(Site==match(max(ranges),ranges))
min_range_wood = wood_rich%>%filter(Site==match(min(ranges),ranges))
stats = rbind(max_wood,min_wood, max_range_wood, min_range_wood)
values = c(max(d),min(d),max_range,min_range)
n = colnames(stats)
n = c(n[-7],"values")
colnames(stats) = n
rownames(stats) = c("max richness","min richness","max range","min range")
#Look at top and bottom quartiles?
u_woods = wood_rich%>%filter(d>100)
l_woods = wood_rich%>%filter(d<60)
good_bad = rbind(u_woods,l_woods) #good_bad used in mappin below
#function to create a list of graphs of raw cf from the vector sitevector
plots = c("plot1","plot2","plot3","plot4","plot5","plot6","plot7",
"plot8","plot9","plot10","plot11","plot12","plot13","plot14","plot15","plot16")
area = c("4","25","50","100","200")
areas = sort(as.numeric(rep(area,16)), decreasing = FALSE)
plot_site = function(sitevector){
#browser()
for (i in 1:length(sitevector)){
site = sitevector[i]
cf_site = cum_rich_all[[site]]
cf_site$plot = plots
melted_cf = melt(cf_site)
melted_cf$area = areas
siteno  = paste("Site",site)
p = ggplot(melted_cf)+
geom_point(aes(x = area, y = value), colour = "black")+
labs(title = siteno)+
ylab("cumulative richness")+
theme(plot.title = element_text(size = 10))
plot_list[[i]] = p
}
return(plot_list)
}
# scatter plots of raw cf data for top 8 woods
up_woods =  wood_rich%>%filter(d>115)
site_to_plot = up_woods$Site
plot_list =list()
plots_all = plot_site(site_to_plot)
pndf("../Results/8Bestcf.png")
# scatter plots of raw cf data for top 8 woods
up_woods =  wood_rich%>%filter(d>115)
site_to_plot = up_woods$Site
plot_list =list()
plots_all = plot_site(site_to_plot)
pdf("../Results/8Bestcf.pdf")
grid.arrange(plots_all[[1]], plots_all[[2]], plots_all[[3]],
plots_all[[4]], plots_all[[5]], plots_all[[6]],
plots_all[[7]],plots_all[[8]], ncol = 2)
#scatter plots of raw cf for worst 8 woods
lo_woods = wood_rich%>%filter(d<45)
site_to_plot = lo_woods$Site
plot_list =list()
plots_all = plot_site(site_to_plot)
pdf("../Results/8worst.pdf")
grid.arrange(plots_all[[1]], plots_all[[2]], plots_all[[3]],
plots_all[[4]], plots_all[[5]], plots_all[[6]],
plots_all[[7]],plots_all[[8]], ncol = 2)
# modelling the best wood only
best_wood = wood_rich[wood_rich$d == max(wood_rich$d),]
x = best_wood$Site
f_bestsite = spec_rich[[x]]
cf_bestsite = cum_rich_all[[x]]
f_bestsite[f_bestsite==0]=NA
cf_bestsite[cf_bestsite==0]=NA
#f_bestsite = f_bestsite[!is.na]
melted_f = melt(f_bestsite)
melted_cf = melt(cf_bestsite)
# particular prep adding plots, removing plot 16 - for plot 83 empty - and remelting
plots = c("plot1","plot2","plot3","plot4","plot5","plot6","plot7",
"plot8","plot9","plot10","plot11","plot12","plot13","plot14","plot15")
cf_bestsite = cf_bestsite[-16,]
cf_bestsite$plot = plots
melted_cf = melt(cf_bestsite)
f_bestsite = f_bestsite[-16,]
melted_f = melt(f_bestsite)
#lets add areas to data.
area = c("4","25","50","100","200")
areas = sort(as.numeric(rep(area,15)), decreasing = FALSE)
melted_cf$area = areas
melted_f$area = areas
##fit every wood and look at coefficients
#prepare data
plots = c("plot1","plot2","plot3","plot4","plot5","plot6","plot7",
"plot8","plot9","plot10","plot11","plot12","plot13","plot14","plot15","plot16")
area = c("4","25","50","100","200")
areas = sort(as.numeric(rep(area,16)), decreasing = FALSE)
fits_all_woods = function(){
coef_df = data.frame(matrix(ncol = 2, nrow = 0))
#prepare data
for (i in 1:103){
#browser()
cf_site = cum_rich_all[[i]]
cf_site$plot = plots
melted_cf = melt(cf_site)
melted_cf$area = areas
# zeros will freak this model out, make them NAs
melted_cf[melted_cf==0]=NA
#model
model_nlme_area_log = lme(log(value)~log(area),random = ~1|plot, data = melted_cf, na.action = na.omit)
#get coefficients
int = model_nlme_area_log$coefficients$fixed[[1]]
slope = model_nlme_area_log$coefficients$fixed[[2]]
row = c(int,slope)
coef_df = rbind(coef_df,row)
}
colnames(coef_df) = c("intercept","slope")
return(coef_df)
}
fits = fits_all_woods()
#max and min slope
maxvars = fits[which(fits$slope == max(fits$slope)),]
minvars = fits[which(fits$slope == min(fits$slope)),]
ggplot()+
geom_abline(data = fits,aes(intercept = intercept, slope = slope), colour = "pink")+
geom_abline(data = maxvars,aes(intercept = intercept, slope = slope), colour = "red" )+
geom_abline(data = minvars,aes(intercept = intercept, slope = slope), colour = "blue" )+
scale_x_continuous(limits = c(0,5))+
scale_y_continuous(limits = c(0,4))+
ggtitle("ln/ln fits for all woodlands")+
annotate("text", x = 3, y = 0.5,
label = "maximum slope 0.43, minumum slope = 0.098")+
labs(x = "ln(area)", y = "ln(richness)")
pdf("../Results/slopes_hist")
ggplot(fits, aes(slope))+
geom_histogram(binwidth = 0.05, col = "black", fill = "grey")+
ggtitle("Distribution of slopes in linear mixed effect ln/ln fit", subtitle = "S = cA^0.25 ish")
dev.off()
knitr::opts_chunk$set(echo = FALSE,message = FALSE,fig.pos = "H" ,comment=NA, fig.align ="centre")
date = format(Sys.Date(), "%B %d %Y")
cat(c(date, ",   word count 3384"))
#clear the workspace
rm(list = ls())
cat("\014")
#setwd("~/Documents/CMEECourseWork/MiniProject/Code")
library(ggplot2)
library(reshape) # both required for the box plots, otherwise they cant all be presented
# on one page and therefore difficult to analyse
library(rpart)
library(rpart.plot)# both required for the decision tree
library(knitr)
library(kableExtra) # for kable stylig options, to hold position on page
#Get the data , enter input CSV file name here, for data in data directory
inputfile = 'SorariaCompact1.csv'
Dataname = strsplit(inputfile, "\\.")[[1]][[1]]
fullfile = paste("../Data",inputfile,sep = '/')
Data = read.csv(fullfile)
speciesnames = as.character(unique(unlist(Data$Species))) # uselful for nameing things
numspecies = summary(Data$Species) # useful for comparisons
#Median imputation
median_replace1 = function(x){
ifelse(is.na(x), median(x,na.rm = TRUE), x)
}
median_replace2 = function(x){
apply(x,2,median_replace1)
}
Imputed_list = lapply(split.data.frame(Data[,2:12], Data$Species), FUN = median_replace2)
#The imputed dataframe is a list with species as the elements, the following sticks it back together with a different name so both optiona are available
temp = do.call(rbind, Imputed_list)
Imputed_df = cbind(Data[1], temp)
# Some algorthms are sensitive to the scale of the data, so here the entire dataframe is scaled
Scaled_df = scale(Imputed_df[-1])
Scaled_df = cbind(Data[1], Scaled_df)
# but this might reduce the dissimilarity to much, so this is a semi-scaled datafrane.
temp = Imputed_df[-c(1,6,7,8,12)]
temp = scale(temp)
Semi_Scaled_df = cbind(Imputed_df[c(1,6:8,12)], temp)
# Model evaluation metrics
accuracy = function(atable){
a = round(sum(diag(atable)/sum(atable)), digits = 2)
return(a)
}
# precision = TP/( rest of that column in conf matrix = the other species id in same class)
precision = function(atable){
p = vector()
items = vector()
no_predictions = dim(atable)[2]
for (i in 1:no_predictions){
items[i] = paste("class",colnames(atable)[i], sep = "_")
p[i] = round(diag(atable)[i]/(sum((atable)[,i])), digits = 2)
}
precisions = cbind(items,p)
colnames(precisions) = c("Class", "Precision")
return(precisions)
}
#sensitivity = TP/ rest of that row = the other classes the algorithm has put species in
sensitivity = function(atable){
s = vector()
no_actuals = dim(atable)[1]
for (i in 1:no_actuals){
s[i] = round(diag(atable)[i]/(sum((atable)[i,])), digits = 2)
}
sensitivities = cbind(rownames(atable),s)
colnames(sensitivities) = c("Species", "Sensitivity")
return(sensitivities)
}
#Data sampling and test/train sets.
#This shuffles and splits the data
shuffle = function(dataset){
splits = list()
set.seed(42)
n = nrow(dataset)
shuffled = dataset[sample(n),]
train = shuffled[1:round(0.7*n),]
test = shuffled[(round(0.7*n)+1):n,]
splits[[1]] = train
splits[[2]] = test
return(splits)
}
#this subsets the data into species
create_train_test = function(dataset){
sets = as.character(unique(dataset[,1]))
train = data.frame()
test = data.frame()
split_data = list()
for (i in 1:length(sets)){
sub = subset(dataset, dataset[,1] == sets[i])
train_temp = shuffle(sub)[[1]]
test_temp = shuffle(sub)[[2]]
train = rbind(train, train_temp)
test = rbind(test, test_temp)
}
split_data[[1]] = train
split_data[[2]] = test
return(split_data)
}
#PS you can check the splits are correct with summary(train$species), summary(test$species)
#summary(maindata$species), this gives numbers in each species.
#to include a cross fold validation repeat above fold times
# performs the k means algorith over 10 repeats, returns BSS/Wss ratio, accuracy and
repeated_kmeans = function(dataset){
metrics_list = list()
accuracy_vector = vector()
ratio = vector()
species_no = data.frame(matrix(ncol = 7))
colnames(species_no) = speciesnames
sens = data.frame(row.names = speciesnames )
prec = data.frame(rownames = speciesnames)
for (i in 1:10){
kmeans_result = kmeans(dataset[-1], 7, 20, iter.max = 50, algorithm = "MacQueen")
ratio[i] = round(kmeans_result$tot.withinss/kmeans_result$totss, digits = 2)
kmeans_conf = table(Imputed_df$Species, kmeans_result$cluster)
accuracy_vector[i] = accuracy(kmeans_conf)
species = diag(kmeans_conf)
species_no = rbind(species_no, species)# just TP
s = sensitivity(kmeans_conf)
sens = cbind(sens, s[,2])
p = precision(kmeans_conf)
prec = cbind(prec,p[,2])
}
metrics_list[[1]] = ratio # wss/bss
metrics_list[[2]] = accuracy_vector #sum TP/no things done
metrics_list[[3]] = species_no[-1,]
metrics_list[[4]] = sens
metrics_list[[5]] = prec
return(metrics_list)
}
# data exploration - box plots
melted = melt(Scaled_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
theme(axis.ticks = element_blank(), axis.text.x = element_blank())
#ggsave('boxplot1.pdf', plot = boxplot1)
# data exploration - box plots
melted = melt(Imputed_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
theme(axis.ticks = element_blank(), axis.text.x = element_blank())
#ggsave("boxplot2.pdf", plot = boxplot2)
#Getting the results for the kmeans
#Imputed df without scaling
Imputed_kmeans = repeated_kmeans(Imputed_df)
#Semi scaled data
#Semi_scaled_kmeans = repeated_kmeans(Semi_Scaled_df)
# fully scaled data
Scaled_kmeans = repeated_kmeans(Scaled_df)
#Display accuaracy for kmeans calcualted above.
acc_df = data.frame(nrow = 2)
acc_df = rbind(Imputed_kmeans[[2]],Scaled_kmeans[[2]])
rownames(acc_df) = c("unstandardized","standardized")
colnames(acc_df) = c("Run 1","Run 2","Run 3","Run 4","Run 5","Run 6","Run 7", "Run 8","Run 9","Run 10")
kable(acc_df, format = "latex", caption = "Accuracy")%>%
kable_styling(latex_options = "hold_position")
# reapeats over the 5 methods and returns confusion matrix for each, plus accuracy for each
repeated_hclust = function(dataset){
conf = list()
metrics_list = list()
accuracy_vector = vector()
dist_methods = c("euclidean", "maximum","manhattan","canberra","minkowski")
for (i in 1:length(dist_methods)){
method = dist_methods[i]
distance = dist(dataset[-1], method = method)
hcluster = hclust(distance, method = "complete")
cluster = cutree(hcluster, k = 7)
conf[[i]] = table(dataset$Species, cluster)
accuracy_vector[i] = accuracy(conf[[i]])
}
accuracy_df = rbind(dist_methods, accuracy_vector)
metrics_list[[1]]=accuracy_df
metrics_list[[2]]=conf
names(metrics_list)=c("Accuracy", "Confusion Matrix")
return(metrics_list)
}
hcluster = repeated_hclust(Imputed_df)
accs = data.frame(nrow = 2)
accs = rbind(hcluster[[1]][1,], hcluster[[1]][2,])
rownames(accs) = c("Distance Method", "Accuracy")
colnames(accs) = c(" ", " "," ", " "," ")
kable(accs, format = "latex", caption = "Accuracy obtained in hierarchical clustering using different distance metrics for non standarized data")%>%
kable_styling(latex_options = "hold_position")
hcluster_scaled = repeated_hclust(Scaled_df)
accs = data.frame(nrow = 2)
accs = rbind(hcluster_scaled[[1]][1,], hcluster_scaled[[1]][2,])
rownames(accs) = c("Distance Method", "Accuracy")
colnames(accs) = c(" ", " "," ", " "," ")
kable(accs, format = "latex", caption = "Accuracy obtained in hierarchical clustering using different distance metrics for standardized data")%>%
kable_styling(latex_options = "hold_position")
kable(hcluster[[2]][[4]], format = "latex", caption = "Confusion matrix for Canberra method using non-standardized data")%>%
kable_styling(latex_options = "hold_position")
kable(hcluster_scaled[[2]][[4]], format = "latex", caption = "Confusion matrix for Canberra method with standardized data")%>%
kable_styling(latex_options = "hold_position")
prec_imp = precision(hcluster[[2]][[2]])
Standardized_prec = prec_imp[,2]
prec_sc = precision(hcluster[[2]][[4]])
Non_standardized_prec = prec_sc[,2]
sens_imp = sensitivity(hcluster[[2]][[4]])
Standardized_sens = sens_imp[,2]
sens_sc = sensitivity((hcluster_scaled[[2]][[4]]))
Non_standardized_sens = sens_sc[,2]
precision_table = cbind(Standardized_prec, Non_standardized_prec)
rownames(precision_table) = c("Class1", "Class2","Class3","Class4","Class5","Class6","Class7")
colnames(precision_table) = c("Standardized", "Unstandardized")
sensitivity_table = cbind(Standardized_sens, Non_standardized_sens)
rownames(sensitivity_table) = c(speciesnames)
colnames(sensitivity_table) = c("Standardized", "Non-standardized")
kable(precision_table, format = "latex", caption = "Precision for standardized and non-standardized data")%>%
kable_styling(latex_options = "hold_position")
kable(sensitivity_table, format = "latex", caption = "Sensitivity for standardized and non-standardized data")%>%
kable_styling(latex_options = "hold_position")
prec_imp = precision(hcluster[[2]][[2]])
Standardized_prec = prec_imp[,2]
prec_sc = precision(hcluster[[2]][[4]])
Non_standardized_prec = prec_sc[,2]
sens_imp = sensitivity(hcluster[[2]][[4]])
Standardized_sens = sens_imp[,2]
sens_sc = sensitivity((hcluster_scaled[[2]][[4]]))
Non_standardized_sens = sens_sc[,2]
precision_table = cbind(Standardized_prec, Non_standardized_prec)
rownames(precision_table) = c("Class1", "Class2","Class3","Class4","Class5","Class6","Class7")
colnames(precision_table) = c("Standardized", "Unstandardized")
sensitivity_table = cbind(Standardized_sens, Non_standardized_sens)
rownames(sensitivity_table) = c(speciesnames)
colnames(sensitivity_table) = c("Standardized", "Non-standardized")
setwd("~/Documents/CMEECourseWork/CMEEMiniProject/Code")
Imputed_sets = create_train_test(Imputed_df)
Imputed_train = Imputed_sets[[1]]
Imputed_test = Imputed_sets[[2]]
tree = rpart(Species~., Imputed_train, method = "class", control = rpart.control(cp = 0.00001))
