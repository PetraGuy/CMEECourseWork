---
title: "Can machine learning be used to identify species of Sorbus"
author: "PetraGuy, Imperial College London"
output:
  pdf_document: 
    df_print: kable
    fig_caption: yes
fontsize: 11pt
header-includes:
  - \usepackage{lineno}
  - \linenumbers
  - \usepackage{setspace}
  - \doublespacing
  - \usepackage{float}
bibliography: bibliography.bib
---
```{r, echo=FALSE}
#This file has been prepared to run from bash script, 
#required files, such as bibliography are moved
#you cannot therefore compile using the knitr button.
#use python run_MiniProject.py from command line
# NB - the best way to render from bash is Rscript -e "rmarkdown::render(MiniProj3.Rmd"
#No need to knitr. This mimics knitr button perfectly. This document has now been changed 
#to suit the two line bash render which produes separate figures and reimports.
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,message = FALSE,fig.pos = "H" ,comment=NA, fig.align ="centre")
```

```{r, echo = FALSE, comment=NA}
date = format(Sys.Date(), "%B %d %Y")
cat(c(date, ",   word count 3384"))
```

\center
![Figure 1. \textit{Sorbus aucuparia}](Sorbusaucuparia.pdf){width=100%, height=100%}\ 
\center
 
```{r, echo =FALSE}
#This creates pdf from command line, note, sensitive to ' or "

#Rscript -e "library(knitr); knit('MiniProj3.Rmd')"

#Rscript -e "library(rmarkdown); render('MiniProj3.md')"
```



```{r}
#clear the workspace
rm(list = ls())
cat("\014")
#setwd("~/Documents/CMEECourseWork/MiniProject/Code")
library(ggplot2)
library(reshape) # both required for the box plots, otherwise they cant all be presented
                # on one page and therefore difficult to analyse
library(rpart)
library(rpart.plot)# both required for the decision tree
library(knitr)
library(kableExtra) # for kable stylig options, to hold position on page
```

```{r}
#Get the data , enter input CSV file name here, for data in data directory
inputfile = 'SorariaCompact1.csv'
Dataname = strsplit(inputfile, "\\.")[[1]][[1]]
fullfile = paste("../Data",inputfile,sep = '/')
Data = read.csv(fullfile)
speciesnames = as.character(unique(unlist(Data$Species))) # uselful for nameing things
numspecies = summary(Data$Species) # useful for comparisons

```

```{r}
#Median imputation 
median_replace1 = function(x){
  ifelse(is.na(x), median(x,na.rm = TRUE), x)
 }

median_replace2 = function(x){
  apply(x,2,median_replace1)
}

Imputed_list = lapply(split.data.frame(Data[,2:12], Data$Species), FUN = median_replace2)
```

```{r}
#The imputed dataframe is a list with species as the elements, the following sticks it back together with a different name so both optiona are available
temp = do.call(rbind, Imputed_list)
Imputed_df = cbind(Data[1], temp)
```

```{r}
# Some algorthms are sensitive to the scale of the data, so here the entire dataframe is scaled
Scaled_df = scale(Imputed_df[-1])
Scaled_df = cbind(Data[1], Scaled_df)

# but this might reduce the dissimilarity to much, so this is a semi-scaled datafrane.

temp = Imputed_df[-c(1,6,7,8,12)]
temp = scale(temp)
Semi_Scaled_df = cbind(Imputed_df[c(1,6:8,12)], temp)

```


```{r}
# Model evaluation metrics

accuracy = function(atable){
  a = round(sum(diag(atable)/sum(atable)), digits = 2)
  return(a)
}
# precision = TP/( rest of that column in conf matrix = the other species id in same class)
precision = function(atable){
  p = vector()
  items = vector()
  no_predictions = dim(atable)[2] 
  for (i in 1:no_predictions){
    items[i] = paste("class",colnames(atable)[i], sep = "_")
    p[i] = round(diag(atable)[i]/(sum((atable)[,i])), digits = 2)
  }
  precisions = cbind(items,p)
  colnames(precisions) = c("Class", "Precision")
  return(precisions)
}
#sensitivity = TP/ rest of that row = the other classes the algorithm has put species in
sensitivity = function(atable){
  s = vector()
  no_actuals = dim(atable)[1] 
    for (i in 1:no_actuals){
    s[i] = round(diag(atable)[i]/(sum((atable)[i,])), digits = 2)
  }
  sensitivities = cbind(rownames(atable),s)
  colnames(sensitivities) = c("Species", "Sensitivity")
  return(sensitivities)
}

```


```{r}
#Data sampling and test/train sets.

#This shuffles and splits the data
shuffle = function(dataset){
  splits = list()
  set.seed(42)
  n = nrow(dataset)
  shuffled = dataset[sample(n),]
  train = shuffled[1:round(0.7*n),]
  test = shuffled[(round(0.7*n)+1):n,]
  splits[[1]] = train
  splits[[2]] = test
  return(splits)
}

#this subsets the data into species
create_train_test = function(dataset){
  sets = as.character(unique(dataset[,1]))
  train = data.frame()
  test = data.frame()
  split_data = list()
  for (i in 1:length(sets)){
    sub = subset(dataset, dataset[,1] == sets[i])
    train_temp = shuffle(sub)[[1]]
    test_temp = shuffle(sub)[[2]]
    train = rbind(train, train_temp)
    test = rbind(test, test_temp)
    }
  split_data[[1]] = train
  split_data[[2]] = test
  return(split_data)
}

#PS you can check the splits are correct with summary(train$species), summary(test$species)
#summary(maindata$species), this gives numbers in each species.

#to include a cross fold validation repeat above fold times

```

```{r}
# performs the k means algorith over 10 repeats, returns BSS/Wss ratio, accuracy and 

repeated_kmeans = function(dataset){ 
  metrics_list = list()
  accuracy_vector = vector()
  ratio = vector()
  species_no = data.frame(matrix(ncol = 7))
  colnames(species_no) = speciesnames
  sens = data.frame(row.names = speciesnames )
  prec = data.frame(rownames = speciesnames)
  for (i in 1:10){
    kmeans_result = kmeans(dataset[-1], 7, 20, iter.max = 50, algorithm = "MacQueen")
    ratio[i] = round(kmeans_result$tot.withinss/kmeans_result$totss, digits = 2)
    kmeans_conf = table(Imputed_df$Species, kmeans_result$cluster)
    accuracy_vector[i] = accuracy(kmeans_conf)
    species = diag(kmeans_conf)
    species_no = rbind(species_no, species)# just TP
    s = sensitivity(kmeans_conf)
    sens = cbind(sens, s[,2])
    p = precision(kmeans_conf)
    prec = cbind(prec,p[,2])
  }
  metrics_list[[1]] = ratio # wss/bss
  metrics_list[[2]] = accuracy_vector #sum TP/no things done
  metrics_list[[3]] = species_no[-1,]
  metrics_list[[4]] = sens
  metrics_list[[5]] = prec
  return(metrics_list)
}
```
\flushleft

#0. Abstract.

There are many reports in the literature of machine learning as a method of identifying plants using visual images of leaf or flowers, for example,[@Gwo,@Val]. However, the use of morphological features is less well documented. Three algorithms were used to separate seven species of \textit{Sorbus} within the subgenus \textit{Soraria} based on morphological measurements of fruit and leaves. Two unsupervised clustering techniques, namely K-means and hierarchical clustering and one supervised decision tree. Box plots showed  considerable overlap of characteristics between the species, but that some species were differentiated in one or two characteristics, suggesting clustering techniques would be less successful than decision tree methods. This was seen in the results with K-means modelling being unstable and unable to repeatedly produce accurate results. Hierarchical clustering using the canberra distance metric gave an accuracy of 0.42 while precision ranged from 0 to 0.81 and sensitivity from 0 to 0.87 using non-standardized data. A decision tree was the most successful method giving an accuracy of 0.68 with precision ranging from 0.4 to 1 and sensitivity from 0.17 to 0.85. 

#1. Introduction - The genus \textit{Sorbus}.

\textit{Sorbus} is a member of the Rosaceae family, perhaps the best known species being \textit{Sorbus aucuparia}, the Rowan or Mountain Ash. However, there are over 50 species of \textit{Sorbus} in the UK, [@NBN], 38 of these are vulnerable or critically endangered and most are endemic or native [@measurements].  There are four diploid species, but, as with many Rosaceae, \textit{Sorbus} produce new apomictic polyploid species, [@robertson]. These can also produce viable pollen and can therefore back-cross with other diploid or polyploid species,[@ludwig2013]. This results in a large number of genetically unique, stable, clonal communities, which can look very similar to each other. This presents a problem with recording and many \textit{Sorbus} require expert knowledge to correctly identify to species level because much of the identification depends on comparative knowledge, [@crib]. This tends to dissuade recorders, or encourages records at aggregate level. This is a problem for such an important genus with many endangered plants that could benefit from identification.

\textit{Sorbus} are grouped into six subgenera, each of which are reasonably easy to identify by recorders with some knowledge. More difficulty arises when identifying plants within these subgenera, and this is where this work has concentrated. In this modelling only the subgenus \textit{Soraria} has been trialed. This subgenus consists of eight species all similar in appearance to \textit{Sorbus intermedia}, although only seven species are considered based on the availability of data. These plants are distinguished from other subgenera by having leaves with rounded lobes which are tomentose beneath and the fruits having fewer lenticles. Perhaps the most noticeable difference between plants within the subgenus are the larger fruits on \textit{S. intermedia}, the smaller leaves of \textit{S. minima} and the small fruits of \textit{S. mougeotii}.


#2. Data and data preparation

The data was provided by Dr T Rich, the Botanical Society of Britain and Ireland expert on \textit{Sorbus} and  consists of leaf and fruit measurements. For the leaves, the length, width, widest point on the leaf, base angle, number of veins, depth of the lobes, and vein angle  have been recorded. For the fruit, the length and the width are used. Due to the variability in leaf size across one plant, the measurements were all carried out in a specific manner described by Rich et al, [@measurements]. Essentially, repeated measurements of the central leaves on sterile spurs on the sunlight side of the tree are recorded and averaged over at least ten leaves. 

The nature of collection means that the data was sparse. Not every plant in each species had a complete set of measurements or the same number of records as other species. For example, \textit{S. intermedia} had 126 observations but \textit{S leyana} only had 39. This is due to the relative occurrence of the two species. \textit{S. intermedia} is a common plant found throughout the UK in easily accessible places, whilst \textit{S. leyana} is only found in two sites in South Wales, sometimes on the sides of cliffs. In addition,  measurements  cannot all be collected at the same time. Leaves must be measured when mature, around flowering time, and therefore cannot be measured in conjunction with fruit. Separate trips to re-measure fruit on the same trees may not be possible. For \textit{S. intermedia}, for example, of 122 records,  72 are purely for fruit measurements and the remaining 50 purely for leaf measurements, and these occur on different plants If imputation was carried out, 59% of the leaf measurements would be imputed, which would be detrimental to the accuracy of the model, [@peters]. Initial data exploration did find  this reduced the accuracy of K-means.

Therefore, the sparsity was handled in two ways. Firstly, by reallocating measurements. For example, the 50 leaf measurements for \textit{S. intermedia} were assigned to 50 fruit measurements and the excess 22 were not used. Secondly, for some species, where there were only a few additional rows of incomplete data, median imputation was carried out. 

Although it seems dubious to assign records from one plant to another, in this analysis this was felt to be acceptable for two reasons. Firstly, this project  focuses on modelling techniques and an initial exploration of machine learning methods; it is not intended as  a complete and accurate method for species identification at this stage. Secondly, the clonal nature of these plants implies that we would expect a great deal of similarity within a species.  However, if these plants are phenotypically very plastic, this assumption may be invalid.  The range of leaf sizes within each plant was not available, so a comparison of the variation within each plant and between all the plants of each species would be useful here. 

This data handling procedure also has the benefit of producing a data-set with no missing values, and some of the machine learning algorithms used here had no method for dealing with these, hence they must be removed before modelling. Clustering algorithms, because they rely on distance metrics, are usually sensitive to scale in the data,[@ismail], therefore the data was also standardized and each clustering model carried out on both standardized and non-standardized data. Since standardization should not effect a decision tree, [@nisbet], only the non-standardized data was modeled.

Because each species had a different number of records a random stratified sampling system was carried out to create train and test sets used in the supervised leaning algorithm in which each species was split into 70/30 train/test sets. 

#3. Modelling

##3.1. Model performance metrics

In supervised learning, the correct and incorrect values assigned to each class are known, and these are used to evaluate the model by calculating accuracy, precision and sensitivity.

Accuracy is is the number of correct values divided by total number of items evaluated. 

Precision is the true positive rate of a predicted class. The precision for a species tells you  how accurately the algorithm is identifying a species, a low precision tells you that other species are incorrectly lumped with the correct species. A high precision tells you that most of the species are correctly identified and that the predicted class will be predominantly made up of the right species.

Sensitivity is the true positive rate of a species. A low number tells you the correct species have been put in other, incorrect, classes. A high sensitivity tells you that most of the species have been put in the right class, and that most of the actual species are in the correct predicted class.

Actually, despite using a mixture of unsupervised and supervised learning methods, we do know the identification of the species, so in fact we can also calculate accuracy, precision and sensitivity for the unsupervised models and hence compare all models using the same metric.

In addition, clustering algorithms can use various other metrics, such as the ratio of within cluster sum of squares to total sum of squares to evaluate the model. For well defined, compact clusters the ratio will be small. Since this metric was not available for all models, it is not used to compare different models. In addition, since the two clustering techniques performed so poorly, there was no reason to compare the two, and therefore these metrics are not shown here.

Confusion matrices, which summarize the the frequencies of the species allocated to different classes and clusters, were produced to examine two of the models, but they were unfeasible for the K-means algorithms since this was repeated ten times, as discussed below, and the large number of confusion matrices would obfuscate the results. Since they give the same information as accuracy, precision and sensitivity, they were used to discuss hierarchical clustering and the decision tree, but not to compare models or examine the results of K-means.

##3.2 Modelling methods.

Three machine learning methods were used   K-means,  hierarchical clustering and a decision tree. The first two being unsupervised clustering techniques and the third a supervised classification algorithm.


###3.2.1.Decision tree.

Variables are used to make binary decisions as whether data points are part of a group or not. Splits are made based on whether the information after the decision, i.e., the separation of the groups, is increased or decreased. The final classes would ideally contain only the items of a single species, this will rarely be the case due to noise within the data. The model here is represented by the logical processes followed to reach the final classes. The rpart package was used for the decision tree, [@rpart]. The rpart library also offers a decision tree plot which summarizes the binary choices used at each node. 

###3.2.2. K-means

K-means is an unsupervised clustering technique. Even though we do  know the identity of the instances in the data, this is not used in the model. Instead, the data is grouped into clusters where the aim is to make the items within each cluster similar, whilst each cluster is as dissimilar as possible from other clusters. This is similar to a classification technique except the classes to which the items belong are not specified. In clustering, no information is needed about the objects and there is no right or wrong, so in that sense, our problem does not demand clustering. We know what species a sample belongs to and we do not want it allocated to another cluster. However, it is a useful technique to see if the model reflects the patterns we know the data contains. The K in K-means refers to the number of clusters to be used, which, because we know the data contains seven species we specified as such.

In K-means  k centroids are randomly assigned to the data. The data points are then assigned to the closest centroid, resulting in k clusters. The centroid is then moved to the average location of the data-points in its cluster. This process is repeated until the centroid position is stable, or the maximum number of iterations has occurred. If repeating the K-means function results in different clusters, which can be seen in differences in accuracy, it can be assumed that the algorithm is not efficient at separating clusters. Since the number of clusters is known, repeating the algorithm and examining the accuracy on each repeat will indicate the success of the model. Ten repeats of the model were carried out and the accuracy calculated on each run.

###3.2.3. Hierarchical Clustering.

Bottom up hierarchical clustering assigns each data-point to a single cluster, the distance between the clusters is calculated and the closest two points are aggregated into a new cluster, so the clusters decrease by one.  The process is repeated until all items are clustered into one. The clusters can be cut at k = 7 and the members can be examined. Hierarchical clustering was explored using different distance methods in order to ascertain the method giving the highest accuracy and this method was then used to calculate precision and sensitivity. The hclust function was used which is part of the stats package which is usually included in base R.

##3.4 Computing languages

R was the main language used in this project within Rstudio, [@R]  although there is no reason, in terms of functionality, why python could not be used. A  benefit of  R was that it can easily be used in conjunction with R markdown, which is used by the data providers and who may want to modify the model.  If the focus of the project had been machine learning itself, the caret package [@caret] would have been more efficient than the methods written here; it provides hundreds of algorithms, training options, plotting and cross fold validation all wrapped up within its functions. The package would allow more thorough and robust models to be produced. 

A bash script was used to compile the markdown file, which has the benefit of allowing neater folders. For example, compiling within Rstudio will not always retrieve images from folders other than the working directory, and therefore the Code directory can become cluttered with non-code files. Using bash means he image files can be stored outside the Code directory and moved in and out when compiling.   

Python was used to run the bash script to full fill the criteria of the project, but this was not necessary and perhaps not in the spirit of using the best tool for the job. A better use for python could have been the initial data wrangling.

 R markdown was used as it provides the same functionality as Latex, allowing the use of Latex commands directly within the document, but with the added benefit of being a dynamic document that is commonly used by other researchers in ecology.  

#4.Data exploration.

If the data is separated into clearly defined groups, we can be sure that a clustering algorithm will work. Box plots are presented for the standardized and non-standardized data and show how the scale and separation between groups could be an issue for the clustering algorithms. The plots also show that certain features are clearly differentiate in certain species. For instance, fruit width would separate \textit{S. anglica}, and then fruit length would subsequently separate \textit{S. leyana}. This suggests that a decision tree algorithm could be successful. 


```{r boxplot1,  fig.cap="Box plots for standardized data" }
# data exploration - box plots
melted = melt(Scaled_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
  theme(axis.ticks = element_blank(), axis.text.x = element_blank())
#ggsave('boxplot1.pdf', plot = boxplot1)
```

\center
![Figure 1. Box plots for standardized data](figure/boxplot1-1.png){width=100%, height=100%}\ 
\center \underline{Figure 1. Box plots for standardized data}. The plots show that certain features do define certain species. For example fruit length or fruit ration separate \textit{S. leyana}, leaf width separates \textit{S. anglica} and leaf ratio separates \textit{S. mougeotti}
\clearpage

\newpage


```{r boxplot2, message = FALSE, fig.cap="Box plots for non-standardized data" }
# data exploration - box plots
melted = melt(Imputed_df)
ggplot(data = melted) +  geom_boxplot(aes(x=Species,y=value, fill = Species)) +   facet_wrap(~variable) +
  theme(axis.ticks = element_blank(), axis.text.x = element_blank())
#ggsave("boxplot2.pdf", plot = boxplot2)
```
\center
![Figure 2 . Box plots for non-standardized data](figure/boxplot2-1.png){width=100%, height=100%}\ 
\center \underline{Figure 2. Box plots for non-standardized data}. The plots show that the scale between the variables is not consistent and therefore you would expect a negative impact on the effectiveness of clustering algorithms

\clearpage

\newpage

\flushleft

#5. Results

Since the output is in the form of tables, these have been placed in the appendices and the results are summarized below.

##5.1 Kmeans

The accuracy shown in table 1 of the appendix I is different on each run implying that the algorithm is not successfully grouping the data into the same clusters. 

Table 2 shows the percentage of each species correctly allocated to its cluster on each of the ten repeats for the  For example, the top row from left to right, gives the true positive rate for \textit{S. anglia} on each subsequent run on the K-means algorithm.

The results again show that the algorithm is not consistently allocating species to the correct cluster. On some runs, it is very accurate for some species, but not necessarily for all the others.  Tables 4 to 7 show that 100% sensitivity and precision could be  achieved on the standardized data for \textit{S. arranensis}, but this was not seen in other species and in a subsequent run this would drop to 0.

In summary, K-means is not consistent across species, does not achieve high accuracy and is not repeatable.

##5.2 Hierarchical Clustering

Tables 8 and 9 in Appendix II show that the Canberra metric gives an accuracy of 0.42 for the non-standardized data. The Euclidean and Minkowski methods give slightly worse accuracy of 0.41 for the standardized data. As in K-mean, the equal performance of the standardized and non-standardized data is contrary to expectations. The sensitivity and precision calculated for standardized and non-standardized data are shown in tables 10 to 13. 

The results are again inconsistent,  a high precision and sensitivity of 0.69 and 0.68 is achieved for \textit{S. anglia} using standardized data, but those values are 0 for \textit{S. Intermedia}.

In summary, the hierarchical clustering technique gives low accuracy and inconsistent precision and sensitivity across the species and classes.

##5.3 Decision Tree

\center
![Figure 3 Decision tree](figure/dectree-1.png){width=100%, height=100%}\ 
\center\underline{Figure 3. rpart plot}

\flushleft

Table 14 of Appendix III shows numbers of species in each group, it is useful to be aware of the proportion of species in the test set when analyzing the tree plot.

The plot shows the decisions on which the tree has been split. The percentage is the percentage of observations in that leaf or node. The leaves represent the predicted class into which the data is split.  The numbers in the leaves and the nodes show the predicted probability for each class.

The plot shows that the first decision splits the data roughly in half depending on the fruit being either greater or less than 11mm wide. The thinner fruit is then predominantly assigned to \textit{S. anglica} based on the length being less than 13mm. 39% of the test set is in this leaf, \textit{S. anglica} comprises 42% of the test set, this allocation is therefore very accurate.  The wider fruits take more decisions to assign the species. Fruit ratio (fruit width/fruit length) and fruit length < 9.8mm gives 14% of the data, most of which is allocated to \textit{S.intermedia} with some \textit{S. cuneifolia}.  The \textit{S. minima} leaf contains only that species (high precision) but only 6% of the data instead of 13% so we can see that around half this species has been incorrectly assigned (low sensitivity).

The accuracy of the tree, shown in table 15, 0.68. The confusion matrix in table 16 shows that most of \textit{S. anglica, S. intermedia, S.leyana, and S.minima} are grouped together. The sensitivity, shown in table 17, is above 43% for 6 out of the 7 species while table 18 shows that the precision is above 50% for 6 classes and 100% for \textit{S. minima}. 

A summary of precision and sensitivity for hierarchical clustering and the decision tree are shown in tables 19 and 20.  The K-means has not been included since the inconsistency of the method demonstrates that it is not suitable for this data.

In summary, the decision tree is the most successful of the algorithms achieving the highest accuracy of 0.68, with precision and sensitivity being consistently higher across the classes and species. 


#6 Conclusion.

K-means was not successful in separating the data into clusters which could be interpreted as species of \textit{Sorbus}. The algorithm was seen to be unrepeatable and the accuracy was always less 0.3.  Sometimes high precision or sensitivity was achieved for a single species, but this was not reflected in the other species and it was not repeatable. The standardized data gave only slightly better results. It is not clear from this analysis whether it is the nature of the data itself that is the cause of the poor performance of this technique; Raykov et al [@raykov] describe the need for data subsets of equal variance and size, which was not the case here. Or the data preparation may have been at fault. Different methods standardization have been shown to influence the outcome of K-means, and that the method used here may not be the optimum, [@steinley]. 

Hierarchical clustering achieved  an accuracy of 0.42 using the Canberra method in non-standardized data and 0.41 using the Euclidean and Minowski method in standardized data. The confusion matrix for the non-standardized data showed better allocation of \textit{S Anglica} but the confusion matrix for standardized data was better for allocating \textit{S mougeotii}. The sensitivity and precision also gave inconsistent results for the standardized and non-standardized data. Neither data treatment being better overall for all species. Overall, hierarchical clustering was not successful, and again, it is not clear whether this is due to the data preparation of the nature of the data. The fact that the non-standardized data sometimes gave better results is unexpected and has not been addressed.

The decision tree method performed more consistently than hierarchical clustering. Although a single species might have a higher sensitivity in clustering, across all species the decision tree performed better, with  five of the seven species achieving greater than 0.6 sensitivity and precision above 0.5 in all but one class. The overall accuracy was also the highest at 0.68.

In conclusion, machine learning using a decision tree algorithm looks to be a successful method for identifying species of \textit{Sorbus}.


#7 Further work

Different methods of standardization could be tried for the unsupervised methods, as well as other clustering algorithms which may be better able to model this data.

All the variables were used in the decision tree, which may not be the best model. Rpart provides information on the importance of variables which can be used to ascertain which can be removed, and this might further improve the performance of the model.

The decision tree model could be extended to include cross fold validation in order to give more robust predictions. Other species of \textit{Sorbus} could be modeled to see if the success was due to the specific morphological characteristics of the \textit{Soraria} subgenus and compare results for other subgenera. 

\textit{Sorbus} are  particularly difficult to identify due to the similarity between species. It would therefore be an interesting comparison to use this model to asses more easily differentiated plants, such as grasses or sedges. These can be problematic to recorders due to the number of features that must be cross-referenced, but these features are more differentiated than in \textit{Sorbus}.

\clearpage

\newpage


```{r}
#Getting the results for the kmeans
#Imputed df without scaling
Imputed_kmeans = repeated_kmeans(Imputed_df) 
#Semi scaled data
#Semi_scaled_kmeans = repeated_kmeans(Semi_Scaled_df)
# fully scaled data
Scaled_kmeans = repeated_kmeans(Scaled_df)
```

#Appendix I K-means results. 

```{r}
#Display accuaracy for kmeans calcualted above.
acc_df = data.frame(nrow = 2)
acc_df = rbind(Imputed_kmeans[[2]],Scaled_kmeans[[2]])
rownames(acc_df) = c("unstandardized","standardized")
colnames(acc_df) = c("Run 1","Run 2","Run 3","Run 4","Run 5","Run 6","Run 7", "Run 8","Run 9","Run 10")
kable(acc_df, format = "latex", caption = "Accuracy")%>%
kable_styling(latex_options = "hold_position")
```


```{r,}
#Display percentage of true positives from the confusion matrix calcualted in kmeans chunk above
m1 = Imputed_kmeans[[3]]
#m2 = Semi_scaled_kmeans[[3]]
m3 = Scaled_kmeans[[3]]
```


```{r}

m1_percent = round(apply(m1, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m1_percent) = c(1:10)
kable(m1_percent, format = "latex", caption = "Percentage of true positives for non-standarized data")%>%
kable_styling(latex_options = "hold_position")
```


```{r, eval = FALSE}
m2_percent = round(apply(m2, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m2_percent) = c(1:10)
kable(m2_percent, format = "latex", caption = "Percentage of true positives for semi-standarized data" )

```


```{r}
m3_percent = round(apply(m3, 1, function(x) (x/numspecies)*100), digits = 2)
colnames(m3_percent) = c(1:10)
kable(m3_percent, format = "latex", caption = "Percentage of true positives for standarized data")%>%
kable_styling(latex_options = "hold_position")


```


```{r}
prec_df_Imputed = Imputed_kmeans[[4]]
colnames(prec_df_Imputed) = c(1:10)
kable(prec_df_Imputed, format = "latex", caption = "Precision of kmeans with non standardized data")%>%
kable_styling(latex_options = "hold_position")
```

```{r}
prec_df = Scaled_kmeans[[4]]
colnames(prec_df) = c(1:10)
kable(prec_df, format = "latex", caption = "Precision of kmeans with standardized data")%>%
kable_styling(latex_options = "hold_position")
```


```{r}
sens_df_Imputed = Imputed_kmeans[[4]]
colnames(sens_df_Imputed) = c(1:10)
kable(sens_df_Imputed, format = "latex", caption = "Sensitivity of kmeans with non standardized data")%>%
kable_styling(latex_options = "hold_position")
```
```{r}
sens_df_scaled = Scaled_kmeans[[4]]
colnames(sens_df_scaled) = c(1:10)
kable(sens_df_scaled, format = "latex", caption = "Sensitivity of kmeans with standardized data")%>%
kable_styling(latex_options = "hold_position")
```

\clearpage

\newpage

```{r}
# reapeats over the 5 methods and returns confusion matrix for each, plus accuracy for each
repeated_hclust = function(dataset){
  conf = list()
  metrics_list = list()
  accuracy_vector = vector()
  dist_methods = c("euclidean", "maximum","manhattan","canberra","minkowski")
for (i in 1:length(dist_methods)){
  method = dist_methods[i]
  distance = dist(dataset[-1], method = method)
  hcluster = hclust(distance, method = "complete")
  cluster = cutree(hcluster, k = 7)
  conf[[i]] = table(dataset$Species, cluster)
  accuracy_vector[i] = accuracy(conf[[i]])
}
  accuracy_df = rbind(dist_methods, accuracy_vector)
  metrics_list[[1]]=accuracy_df
  metrics_list[[2]]=conf
  names(metrics_list)=c("Accuracy", "Confusion Matrix")
  return(metrics_list)

}
```

#Appendix II Hierachical clustering results

```{r}

hcluster = repeated_hclust(Imputed_df)
accs = data.frame(nrow = 2)
accs = rbind(hcluster[[1]][1,], hcluster[[1]][2,])
rownames(accs) = c("Distance Method", "Accuracy")
colnames(accs) = c(" ", " "," ", " "," ")
kable(accs, format = "latex", caption = "Accuracy obtained in hierarchical clustering using different distance metrics for non standarized data")%>%
kable_styling(latex_options = "hold_position")

```
```{r}
hcluster_scaled = repeated_hclust(Scaled_df)
accs = data.frame(nrow = 2)
accs = rbind(hcluster_scaled[[1]][1,], hcluster_scaled[[1]][2,])
rownames(accs) = c("Distance Method", "Accuracy")
colnames(accs) = c(" ", " "," ", " "," ")
kable(accs, format = "latex", caption = "Accuracy obtained in hierarchical clustering using different distance metrics for standardized data")%>%
kable_styling(latex_options = "hold_position")

```




```{r}
kable(hcluster[[2]][[4]], format = "latex", caption = "Confusion matrix for Canberra method using non-standardized data")%>%
kable_styling(latex_options = "hold_position")
```



```{r}
kable(hcluster_scaled[[2]][[4]], format = "latex", caption = "Confusion matrix for Canberra method with standardized data")%>%
kable_styling(latex_options = "hold_position")
```



```{r}
prec_imp = precision(hcluster[[2]][[2]])
Standardized_prec = prec_imp[,2]
prec_sc = precision(hcluster[[2]][[4]])
Non_standardized_prec = prec_sc[,2]
sens_imp = sensitivity(hcluster[[2]][[4]])
Standardized_sens = sens_imp[,2]
sens_sc = sensitivity((hcluster_scaled[[2]][[4]]))
Non_standardized_sens = sens_sc[,2]

precision_table = cbind(Standardized_prec, Non_standardized_prec)
rownames(precision_table) = c("Class1", "Class2","Class3","Class4","Class5","Class6","Class7")
colnames(precision_table) = c("Standardized", "Unstandardized")

sensitivity_table = cbind(Standardized_sens, Non_standardized_sens)
rownames(sensitivity_table) = c(speciesnames)
colnames(sensitivity_table) = c("Standardized", "Non-standardized")
```


```{r}
kable(precision_table, format = "latex", caption = "Precision for standardized and non-standardized data")%>%
kable_styling(latex_options = "hold_position")
```



```{r}
kable(sensitivity_table, format = "latex", caption = "Sensitivity for standardized and non-standardized data")%>%
kable_styling(latex_options = "hold_position")
```

\clearpage

\newpage

#Appendix III Decision tree results

```{r}

Imputed_sets = create_train_test(Imputed_df)
Imputed_train = Imputed_sets[[1]]
Imputed_test = Imputed_sets[[2]]
tree = rpart(Species~., Imputed_train, method = "class", control = rpart.control(cp = 0.00001))
```


```{r dectree, fig.cap="Decision tree"}
rpart.plot(tree, box.palette = "Blues",fallen.leaves = FALSE,  gap=0, space=0)
```


```{r}
speciespropn = round(numspecies/380, digits = 2)
kable(speciespropn, format = "latex", caption = "Proportions of each species in test set")%>%
kable_styling(latex_options = "hold_position")
```



```{r}
pred_tree = predict(tree, Imputed_test, type = "class")
confusion_tree = table(Imputed_test$Species, pred_tree)
#cat("The accuracy for the decision tree is ")
#cat(accuracy(table(Imputed_test$Species, pred_tree)))
tree_acc = accuracy(table(Imputed_test$Species, pred_tree))
kable(tree_acc, format = "latex", caption = "Accuracy of the decision tree")%>%
kable_styling(latex_options = "hold_position")
```
```{r, fig.cap="The confusion matrix details exactly how the species were placed"}
kable(cbind(table(Imputed_test$Species, pred_tree),summary(Imputed_test$Species)), format = "latex", caption =  "Confusion matrix for the decision tree")%>%
kable_styling(latex_options = "hold_position")
```

```{r, fig.cap="Sensitivity obtained using decision tree"}
sens_tree = (sensitivity(table(Imputed_test$Species, pred_tree)))
kable(sens_tree, format = "latex", caption = "Sensitivity for the decision tree")%>%
kable_styling(latex_options = "hold_position")
```


```{r, fig.cap="Precision obtained using decision tree"}
prec_tree = precision(table(Imputed_test$Species, pred_tree))
kable(prec_tree, format = "latex", caption = "Precision for the decision tree")%>%
kable_styling(latex_options = "hold_position")

```

\clearpage
\newpage

#Appendix IV Comparison of hierachical clustering and decision tree
```{r}
#Summarise all the precisions and sensitivity
sens_totals = cbind(sensitivity_table, sens_tree[,2])
colnames(sens_totals) = c("hclust standardised", "hclust non-standardized", "tree")
kable(sens_totals, format = "latex", caption = "Sensitivity for hierarchical clustering and decision tree")%>%
kable_styling(latex_options = "hold_position")
```


```{r}
prec_totals = cbind(precision_table, prec_tree[,2])
colnames(prec_totals) = c("hclust standardised", "hclust non-standardized", "tree")
kable(prec_totals, format = "latex", caption = "Precision for hierarchical clustering and decision tree")%>%
kable_styling(latex_options = "hold_position")
```

\clearpage

\newpage

#References

\flushleft
